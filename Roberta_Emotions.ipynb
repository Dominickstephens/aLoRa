{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/main/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets accelerate evaluate pynvml\n",
    "!pip install -U peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip show peft transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBk7G5ubZBy1",
    "outputId": "adabc39b-917c-45f3-fde8-e8688abd0f3d"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml) (12.575.51)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n",
      "Name: peft\n",
      "Version: 0.17.1\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AdaLoraConfig\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "NUM_LABELS = 28\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "FF_LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "# https://arxiv.org/pdf/2412.12148\n",
    "THRESHOLD = 0.5"
   ],
   "metadata": {
    "id": "_jAa-D4aZDe6"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "max_length = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    encodings['labels'] = batch['labels']\n",
    "    return encodings\n",
    "\n",
    "ds_encoded = ds.map(tokenize, batched=True)\n",
    "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
    "        if item['labels'] is not None:\n",
    "            for l in item['labels']:\n",
    "                if 0 <= l < NUM_LABELS:\n",
    "                    multi_hot[l] = 1.0\n",
    "        labels.append(multi_hot)\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluation Metric\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
   ],
   "metadata": {
    "id": "tbB9oI1PZGHc",
    "outputId": "951aaeca-4c48-45da-e985-c1fd9a806e3f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "90ba8a759b664dfda3cfcc8e96e1824c",
      "54fac8f0f2ad44b3832ae6b1fc1fc9ad",
      "45a63038871c48f6915051bcb91dcb1e",
      "778ede78f5f64a738bb6fb7747bbce4d",
      "cdff8046e7e842ba942693b462969077",
      "da62cbb63d864152853261c1bd378444",
      "71a241d76fc54c459dec914d8bcdac42",
      "a1e8975fcc594d1eaa5ba984eaeb0593",
      "59322390d0184e18b03e973fb028f121",
      "64ef48ff2c14481aa640b595d0b15782",
      "448a0a8e316341cdbc9a56e44918322b",
      "8c48b9581e8448b4a5c6cf094cd4f5d3",
      "7bda0c0e751c4c2caf7daccc7cf191e8",
      "ee56505ab0ad4a3795c84b988928819e",
      "283f6f9efcb84e10ae1dd2b1bc8dbeda",
      "5aa302ae2f9e4d9cbd1f71f77478c16b",
      "16b3f3d7a0554f318d4bcd0519e98632",
      "0eff9cdcf0584ee5b442ce75a64fdcb0",
      "85a4eb3e4f1245379b46538bdebe053d",
      "0a9fb0317bc8414f9d927ed762d9f42d",
      "620dc35910254301a4fa3ef7b4fe1bc2",
      "bc63e327b9de4e8a83e8d76fe246d24d",
      "832d5c7213374c6cbb43cc0d0bd8a824",
      "5216f4387f724876b7e89a96c2d1e906",
      "95fcd1793d4447fba4c1ac5b5a3e2f73",
      "76e83dc02b8145e2a97c3e655bc38223",
      "1b79fa1645064847b96fde341d81449b",
      "44aace7d00b1462e8e90fb9c3cd37bdd",
      "69088e859a384eafb4be533bc4cf5d6a",
      "9914bb7a54c14a249beb07a27be2efa1",
      "98aeee9bd80e4595bf5304f237ceb1da",
      "8d349396097e43829a63fd031cefd5ef",
      "b4392157896f4d25824e51f69a8dbdcd",
      "a8e239d9ef054a92a8f31040cb251676",
      "a566105fed954845b8c8f25ed47b7117",
      "f5faceaa44fb4c73a66491d8651a79c3",
      "dd7322cecabc42b6a665fdb53f002317",
      "ea33e7fd277a492e85ed44d9fd5381c2",
      "f950835952a94a609703ff3a0a4c88d4",
      "c8fa04dc825f44ffbee3113c3ee1ef0d",
      "c41cb9958a8845f4903e11eb931890a3",
      "afb05840069a49729aa102d755d6a9f2",
      "daf175c47d7a407db279283aafd3d774",
      "69b2d7013f224b14a002cc93362d0c6e",
      "0dd3bc708e63496ab1dbf3ea287cefa0",
      "2a5fe6b388cb45869b275c4884b1dacc",
      "40b5ea6493fe417a9c23bf7a50eefa14",
      "ece4991124e241a5a5146d2063075177",
      "922fb116f03849399f0bed9a45a29df2",
      "9ae72c5519724f7eaa1be126c7cd3d94",
      "5f4ea57656484decba963f8595a6d4a8",
      "844ad82127c74a94b5a615d26d028768",
      "4a5e683fd67a471fb0bfe1033144ac8c",
      "891e51687c7c4ee6a23f4c0e93f6eb77",
      "9148d329136645ba9c78e64421ed9cfc",
      "6030b38a3be64e719de1cc6e8ee5f0e6",
      "b438ae21feab42e58a919b850587083d",
      "f573d0b2857e4f1ea1a1d16f526b826d",
      "62d8a951d95948b3ab08f3f33db9d278",
      "4d094f3343f94b94a1e8dc7f9a0b2bf2",
      "5a4493eed53b4ba3a55b2fe061f22758",
      "c014385635304bc18bba9158612d091e",
      "174966e357d04be2acc17b0091871ed4",
      "dae1b675a9234d0c85775876063238d7",
      "a82e15b176ab43759ab7271dca74f749",
      "9a95deddca4541659da23d9c53657f2c",
      "65c1fc32de55448d87a1a3b7c6b50c66",
      "87fc1f17051d4f69990e53fd6cb87ded",
      "2d95eb3328074c91a5ca9c9d023f615e",
      "9dc6ac0ef9d64ac994c3191f41ab5667",
      "362ec86c665a4b76b6ea867782f765a3",
      "f05b6a19bcbc4aca916d3f61af3a2bb2",
      "cbd0c6186b7643b4bbc446d1d0fe308f",
      "d62168c17a5c4d458aee57bb3869b43b",
      "9651144b6be6456b9590f86ba5b9b441",
      "64de3fd5f48747bab9c4e954b690f456",
      "4eb508c4294c41ebbed5f3e9eb2ef70e",
      "2670176569b943ae9f8f2990e6941a9c",
      "e55035f573ef45689e071d0749e60677",
      "e3e99b3c584740c8ac0b2be3a0352c12",
      "ddfaae7fade84bf697caecdfec75ee79",
      "f777a0f95ae04f849c5a7550ab29da88",
      "a958594e4e294415bfc444fe7b4f88bf",
      "995bf9edceeb409b85f4b6ca7f20ea61",
      "7bbcf490edcf4ac5a8386e20d72001a0",
      "d7b2072a986943f4b184114a6ce9c0c5",
      "85eb39ce003d4244b7576b4f7cd61d03",
      "fa703a9020a64f408be9d105f35f140c",
      "05846a806ab749d89170c8e91535c95a",
      "dd73bc72f5ad44519b5ed79db5333c7b",
      "76d3e41f4c404158ac66d9d5cd70dd07",
      "5e3531e172574eb9b0d109215ae1a770",
      "9f66c3318c6e47e8ae11fa40352cb9fb",
      "23547d1699f94d09952bbb7966fff594",
      "973c1e87aca34503bf38efd1240d5fc3",
      "8af839620e3d4d12b1d70bff5ceec13e",
      "cf6c05527bd64558adae1d3e52d5bc64",
      "3f14f9beda1f4c9b9745a3c3d00c9c84",
      "cc6540218051424da4848e54bca900a5",
      "ce262e3983204d1d830c0b2dda4c1097",
      "2454f814ab734fa0ae80d475f1b85652",
      "a0c0dde6dbb54f1eb1e4c9c1b519a734",
      "62b48f9b690c4535ac9c325ab637a942",
      "b60f6a3f2b014306a78ef7ed817e816d",
      "020bc1948c8c4945b0ee1622c877d3cd",
      "c298f15b05014d86a0c2235bb9c612cc",
      "2f882e23e56848888652f3e39af0a132",
      "379e284b662544c7889caf5fb82778c3",
      "2001386e03b144a7a91746b083cec895",
      "ae97e796fb4b4a69bb339bfc99621f7b",
      "8785b022e9934abcba931a4e63509fb2",
      "63c72719054e485a97756f5432fd35b3",
      "0ec2cbf4c56645b5a8961e8f17970d4f",
      "56f70ba46fd54d87864be7a2ab1b6b85",
      "25ed4f62ddb049fd975398c3f92f31b9",
      "eab19ae4266d4d99a3780d82c8b3ad01",
      "6fa53cf9670b4a83921625bebb829b4f",
      "1460fe8bf41049e29c8a7c3c0f735c64",
      "9fab80ebf7384f43b487cd8d7cba6f39",
      "833abcf1e02641c19aeae4c54a13def3",
      "d809ad760c7247418d76ccca24961213",
      "48990465fd5e49949593d42acacbf259",
      "f38a2d78817f4dc6ac29ab0935eb648a",
      "967beac18b9d4a5bb94dde87614f6be1",
      "d04f6a1f4584412e9e2679af910ff53f",
      "d9a9ea49091f4e3cbe0abafcfd82081f",
      "7cd9b7379e1d4b679a86bf4b81c14634",
      "130ebe43db834a318f72591c17f494d5",
      "8089c40e1da240a384200c99876dc838",
      "1cd233b1d24f4546b6584a8afdc2866f",
      "e400d799615b458ab83a83b0725b5ae2",
      "9beac4da5e6f4616b8cee46ab7ef7629",
      "9fbc01c287d74dc1b544526022f424eb",
      "4e58aff66285498eb15f00a7435ba9b1",
      "c4e6172fbcfd431c81fe9bc25bf66129",
      "cf7f2c8770904ea58751e9767849a06b",
      "c768205979a744f7b13c79ff76050dfa",
      "41f3eefb56b042b5afe390d4c2f54a4c",
      "0c78620541404c6c825e5855bff79a85",
      "3c4ae09719094662bac720bff1fa82a5",
      "d95e2cd470d444cf8eff2ce5fb8362b2",
      "7dbba15a3be04d889d3fab2b54b3f61d",
      "07a03b05fbe44f3aafaf2e2bc5f28218",
      "1f20a2e4cbcb450098df509e49672048",
      "c2ccde0e0f7b4b8bada9476a42cbf9d4",
      "d8ca2571929f48f39d96458075fe6afd",
      "63f2967dccfd4befb0d941564f6c34b4",
      "70e27436387e4ac08cf915913888b36f",
      "cf2c24bf59bd4e37ae95eee21c663ec0",
      "e49d5ca03b3a4b52b925a734906ba8ae",
      "b8589f062e0d45ccba386f729fcfe08d",
      "48d8a45cf23e41e9a8c370a90f62a6db",
      "7afa66bcbad44e729c95fca18fb0acfb",
      "d211a8f9d19144da94f1534c9c0aeece",
      "57ffde7e938a44dca3d9dc7373131c2c",
      "f0cb3f34924e4ea4b18d9afd02b08ced",
      "e11fab59eddb4907bd28e322ec104fed",
      "efdbd45a36f3412ca49edc3f21e60ca7",
      "7c8a2e20888847edb4db88f008cc5064",
      "c306eae7fc8e49e8a771985b84a9eba5",
      "8a9ae61e4fd9463b9a15378ed3069516",
      "38769d5b8b5e49d18640119ca8b5d831",
      "618b3afab1994c648c1e28f999b88bd2",
      "795413cd7ad44f369199e0fd5e6a2c7d",
      "7357f17fd0b44c34971cb10e0340521e",
      "795b701123f3451ea2e1332af847a45e",
      "ac22cd3b4a6c4a4a8f65bc189dfc2a1f",
      "4787ba10082e4061bf5f94f1822955a9",
      "9f184fa676c1452b9a1eb1107eea4809",
      "2f4fc004a7024b72809e4194cb6e2247",
      "789125618a094fb393a734d5cdd2e297",
      "7869e5de4a7d4b908ab4623344c8d3d0",
      "83aaaba1901c4a208cc8c7a1e28df8fe",
      "45624e94fc6540d8b213e9b3963b30d3",
      "e77be10007824a389f1ced609d5001f7",
      "3da15c886ff34419ab5dbc1d14ff8c0a"
     ]
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90ba8a759b664dfda3cfcc8e96e1824c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c48b9581e8448b4a5c6cf094cd4f5d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/validation-00000-of-00001.par(\u2026):   0%|          | 0.00/350k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "832d5c7213374c6cbb43cc0d0bd8a824"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8e239d9ef054a92a8f31040cb251676"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dd3bc708e63496ab1dbf3ea287cefa0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6030b38a3be64e719de1cc6e8ee5f0e6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65c1fc32de55448d87a1a3b7c6b50c66"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2670176569b943ae9f8f2990e6941a9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05846a806ab749d89170c8e91535c95a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce262e3983204d1d830c0b2dda4c1097"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8785b022e9934abcba931a4e63509fb2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48990465fd5e49949593d42acacbf259"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fbc01c287d74dc1b544526022f424eb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f20a2e4cbcb450098df509e49672048"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57ffde7e938a44dca3d9dc7373131c2c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "795b701123f3451ea2e1332af847a45e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_model(method: str):\n",
    "    \"\"\"\n",
    "    Prepares a RoBERTa model for fine-tuning using different PEFT strategies:\n",
    "    - \"Full\"      : full fine-tuning\n",
    "    - \"LoRA\"      : standard LoRA\n",
    "    - \"LoRA+\"     : LoRA with Rescaled Stable adaptation\n",
    "    - \"AdaLoRA\"   : Adaptive LoRA (dynamic rank allocation)\n",
    "    - \"DoRA\"      : Weight-decomposed LoRA (Meta 2024)\n",
    "    - \"QLoRA\"     : 4-bit quantized LoRA\n",
    "    \"\"\"\n",
    "\n",
    "    # Load Quantization Config\n",
    "    quantization_config = None\n",
    "    if method == \"QLoRA\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    # Load base RoBERTa\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\" if method == \"QLoRA\" else None,\n",
    "    )\n",
    "\n",
    "    # Choose PEFT variant\n",
    "    if method == \"LoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing LoRA\")\n",
    "\n",
    "    elif method == \"LoRA+\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_rslora=True,  # LoRA+\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\u2699\ufe0f Using LoRA+\")\n",
    "\n",
    "    elif method == \"AdaLoRA\":\n",
    "        config = AdaLoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            init_r=8,\n",
    "            target_r=4,\n",
    "            tinit=100,\n",
    "            tfinal=500,\n",
    "            deltaT=10,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            total_step=len(train_loader) * EPOCHS\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing AdaLoRA\")\n",
    "\n",
    "    elif method == \"DoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_dora=True,  # enables DoRA\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing DoRA\")\n",
    "\n",
    "    elif method == \"QLoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing QLoRA (4-bit quantized + LoRA)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nFull Fine-Tuning (no adapters)\")\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100%)\")\n",
    "        model.to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
    "        return model, optimizer\n",
    "\n",
    "    # Shared setup for PEFT variants\n",
    "    model.print_trainable_parameters()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    return model, optimizer\n"
   ],
   "metadata": {
    "id": "1jfdNdbbZHvC"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, optimizer, method: str, train_loader, device, epochs, save_model):\n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    gpu_utilization_history = []\n",
    "    max_gpu_mem_allocated = 0.0\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(device.index if device.index is not None else 0)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if device.type == 'cuda':\n",
    "                util = nvmlDeviceGetUtilizationRates(handle)\n",
    "                gpu_util_percent = util.gpu\n",
    "                gpu_utilization_history.append(gpu_util_percent)\n",
    "\n",
    "                allocated_mem_bytes = torch.cuda.memory_allocated(device)\n",
    "\n",
    "                max_gpu_mem_allocated = max(max_gpu_mem_allocated, allocated_mem_bytes / (1024**3))\n",
    "\n",
    "                loop.set_postfix(\n",
    "                    loss=loss.item(),\n",
    "                    gpu_util_perc=f\"{gpu_util_percent}%\",\n",
    "                    gpu_mem_gb=f\"{allocated_mem_bytes / (1024**3):.2f}\"\n",
    "                )\n",
    "            else:\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    total_train_time = time.time() - start_time\n",
    "    results['train_time_sec'] = total_train_time\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        results['max_gpu_mem_gb'] = max_gpu_mem_allocated\n",
    "        results['max_gpu_percent'] = max(gpu_utilization_history) if gpu_utilization_history else 0\n",
    "        results['average_gpu_percent'] = sum(gpu_utilization_history) / len(gpu_utilization_history) if gpu_utilization_history else 0\n",
    "\n",
    "        nvmlShutdown()\n",
    "\n",
    "\n",
    "    if save_model:\n",
    "      cpt_str = method + \"_checkpoint.pth\"\n",
    "      torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, cpt_str)\n",
    "\n",
    "    return model, results"
   ],
   "metadata": {
    "id": "Kv5EoKDDcwcN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    hamming_loss,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred_probs = torch.sigmoid(logits)\n",
    "            pred_labels = (pred_probs > threshold).long()\n",
    "\n",
    "            all_probs.append(pred_probs.cpu().numpy())\n",
    "            all_preds.append(pred_labels.cpu().numpy())\n",
    "            all_targets.append(labels.long().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    probs = np.concatenate(all_probs, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Multi-label metrics\n",
    "    results = {\n",
    "        \"f1_macro\": f1_score(targets, preds, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(targets, preds, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(targets, preds, average=\"weighted\"),\n",
    "        \"precision_macro\": precision_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"hamming_loss\": hamming_loss(targets, preds),\n",
    "        \"exact_match_accuracy\": np.mean([np.all(p == t) for p, t in zip(preds, targets)]),\n",
    "    }\n",
    "\n",
    "    # Probabilistic metrics (optional)\n",
    "    try:\n",
    "        results[\"roc_auc_macro\"] = roc_auc_score(targets, probs, average=\"macro\")\n",
    "        results[\"pr_auc_macro\"] = average_precision_score(targets, probs, average=\"macro\")\n",
    "    except ValueError:\n",
    "        results[\"roc_auc_macro\"] = None\n",
    "        results[\"pr_auc_macro\"] = None\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"| {method} Evaluation Results |\")\n",
    "    print(\"-\" * 50)\n",
    "    for k, v in results.items():\n",
    "        if v is not None:\n",
    "            print(f\"{k.replace('_',' ').title():30}: {v:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "id": "YMmQhmOoe9Nh"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cleanup(full_model=None, full_optimizer=None):\n",
    "    if full_model is not None:\n",
    "        del full_model\n",
    "    if full_optimizer is not None:\n",
    "        del full_optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "RSq1mQm1sLod"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_table = []\n",
    "\n",
    "print(\"Before FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating model before training...\")\n",
    "pretrain_eval = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    \"Full Fine-Tuning (Before Training)\"\n",
    ")\n",
    "\n",
    "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
    "results_table.append({\"Method\": \"Before Training\", **before_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24TkGpNProMd",
    "outputId": "d4170676-bde7-466a-8fc1-12a9aed70c63"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before FINE-TUNING\n",
      "============================================================\n",
      "\n",
      "Full Fine-Tuning (no adapters)\n",
      "Total Parameters: 124.67M\n",
      "Trainable Parameters: 124.67M (100%)\n",
      "\n",
      "Evaluating model before training...\n",
      "--------------------------------------------------\n",
      "| Full Fine-Tuning (Before Training) Evaluation Results |\n",
      "--------------------------------------------------\n",
      "F1 Macro                      : 0.0509\n",
      "F1 Micro                      : 0.0770\n",
      "F1 Weighted                   : 0.1676\n",
      "Precision Macro               : 0.0290\n",
      "Recall Macro                  : 0.7075\n",
      "Hamming Loss                  : 0.6950\n",
      "Exact Match Accuracy          : 0.0000\n",
      "Roc Auc Macro                 : 0.5010\n",
      "Pr Auc Macro                  : 0.0444\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "full_method = \"Full Fine-Tuning\"\n",
    "\n",
    "full_model, full_optimizer = prepare_model(full_method)\n",
    "\n",
    "train_full_model = True\n",
    "\n",
    "if (train_full_model):\n",
    "  # Train\n",
    "  full_model, full_train_results = train_model(\n",
    "      full_model,\n",
    "      full_optimizer,\n",
    "      full_method,\n",
    "      train_loader,\n",
    "      DEVICE,\n",
    "      EPOCHS,\n",
    "      save_model=True\n",
    "  )\n",
    "else:\n",
    "  # Load checkpoint\n",
    "  cpt_string = full_method + \"_checkpoint.pth\"\n",
    "  checkpoint = torch.load(cpt_string)\n",
    "  full_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  full_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  results = {}\n",
    "  results['train_time_sec'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "full_eval_results = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    full_method\n",
    ")\n",
    "\n",
    "full_results = {**full_train_results, **full_eval_results}\n",
    "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)"
   ],
   "metadata": {
    "id": "nOh-mU-ycSjn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "outputId": "903dfc45-4d67-49b0-8ebe-44901f98b65e"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Full Fine-Tuning (no adapters)\n",
      "Total Parameters: 124.67M\n",
      "Trainable Parameters: 124.67M (100%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Full Fine-Tuning Epoch 1:  10%|\u2588         | 276/2714 [00:23<03:27, 11.72it/s, gpu_mem_gb=2.37, gpu_util_perc=92%, loss=0.202]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-353256395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_full_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   full_model, full_train_results = train_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mfull_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mfull_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2067776458.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, method, train_loader, device, epochs, save_model)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "del lora_model\n",
    "del lora_optimizer\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "eZAoWflrXvFh"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# MULTI-RUN LoRA / QLoRA / AdaLoRA / DoRA BENCHMARK\n",
    "# ============================================================\n",
    "# import torch\n",
    "\n",
    "methods_to_run = [\"DoRA\"]\n",
    "# results_table = []\n",
    "\n",
    "for lora_method in methods_to_run:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Starting Fine-Tuning with {lora_method}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    train_lora_model = True\n",
    "\n",
    "    # ---- Prepare model and optimizer ----\n",
    "    try:\n",
    "        lora_model, lora_optimizer = prepare_model(lora_method)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to prepare {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if train_lora_model:\n",
    "        # ---- Train ----\n",
    "        try:\n",
    "            lora_model, lora_train_results = train_model(\n",
    "                lora_model,\n",
    "                lora_optimizer,\n",
    "                lora_method,\n",
    "                train_loader,\n",
    "                DEVICE,\n",
    "                EPOCHS,\n",
    "                save_model=True\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"Skipping {lora_method} (Out of memory)\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed for {lora_method}: {e}\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        # ---- Load checkpoint ----\n",
    "        cpt_string = f\"{lora_method}_checkpoint.pth\"\n",
    "        checkpoint = torch.load(cpt_string)\n",
    "        lora_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lora_optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        loss = checkpoint[\"loss\"]\n",
    "        lora_train_results = {\"train_time_sec\": 0, \"final_loss\": loss}\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    try:\n",
    "        lora_eval_results = evaluate_model(\n",
    "            lora_model,\n",
    "            val_loader,\n",
    "            f1_metric,\n",
    "            THRESHOLD,\n",
    "            DEVICE,\n",
    "            lora_method\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed for {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ---- Merge and store results ----\n",
    "    lora_results = {**lora_train_results, **lora_eval_results}\n",
    "    results_table.append({\"Method\": lora_method, **lora_results})\n",
    "\n",
    "    # ---- Cleanup GPU memory ----\n",
    "    del lora_model\n",
    "    del lora_optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to DataFrame for nice display (optional)\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results_table)\n",
    "display(results_df)\n"
   ],
   "metadata": {
    "id": "4EfsUIe4rwN9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "outputId": "01c27fa1-d582-4195-b8d0-0667ac4e1231"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting Fine-Tuning with DoRA\n",
      "======================================================================\n",
      "\n",
      "Using DoRA\n",
      "trainable params: 925,468 || all params: 125,592,632 || trainable%: 0.7369\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DoRA Epoch 1:   2%|\u258f         | 51/2714 [00:03<03:09, 14.03it/s, gpu_mem_gb=9.11, gpu_util_perc=91%, loss=0.269]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3594587342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# ---- Train ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             lora_model, lora_train_results = train_model(\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mlora_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2067776458.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, method, train_loader, device, epochs, save_model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 loop.set_postfix(\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0mgpu_util_perc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{gpu_util_percent}%\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mgpu_mem_gb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{allocated_mem_bytes / (1024**3):.2f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\\nCOMPARISON OF RESULTS\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "# Print table header with new metrics\n",
    "print(f\"| {'Method':<20} | {'Trainable Params (M)':<20} | {'Train Time (s)':<15} | \"\n",
    "      f\"{'Macro F1':<10} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} |\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print each row from results_table\n",
    "for result in results_table:\n",
    "    method_name = result[\"Method\"]\n",
    "    trainable_params = result.get(\"trainable_params\", 0.0)\n",
    "    train_time = result.get(\"train_time_sec\", 0.0)\n",
    "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
    "    accuracy = result.get(\"accuracy\", 0.0)\n",
    "    precision = result.get(\"precision_macro\", 0.0)\n",
    "    recall = result.get(\"recall_macro\", 0.0)\n",
    "\n",
    "    print(f\"| {method_name:<20} | {trainable_params / 1e6:<20.4f} | {train_time:<15.2f} | \"\n",
    "          f\"{f1_macro:<10.4f} | {accuracy:<10.4f} | {precision:<10.4f} | {recall:<10.4f} |\")\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n"
   ],
   "metadata": {
    "id": "5ciWQIO_rzhl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "92245e9d-3ba2-426d-e10d-0a58281ddc14"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "COMPARISON OF RESULTS\n",
      "############################################################\n",
      "| Method               | Trainable Params (M) | Train Time (s)  | Macro F1   |\n",
      "---------------------------------------------------------------------------\n",
      "| Before Training      | 0.0000               | 0.00            | 0.0578     |\n",
      "| LoRA                 | 0.0000               | 1780.28         | 0.4030     |\n",
      "| LoRA+                | 0.0000               | 1777.49         | 0.4094     |\n",
      "| AdaLoRA              | 0.0000               | 2583.87         | 0.3962     |\n",
      "| DoRA                 | 0.0000               | 2143.37         | 0.4131     |\n",
      "############################################################\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract plotting data from results_table\n",
    "methods = [r[\"Method\"] for r in results_table]\n",
    "params = [r.get(\"trainable_params\", 0.0) / 1e6 for r in results_table]  # in millions\n",
    "train_times = [r.get(\"train_time_sec\", 0.0) for r in results_table]\n",
    "f1_macros = [r.get(\"f1_macro\", 0.0) for r in results_table]\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "#  Plot 1: Training Time vs F1\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(train_times[i], f1_macros[i], s=120, label=method,\n",
    "                color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(train_times[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Training Time (seconds)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Training Time vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Parameters vs F1\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(params[i], f1_macros[i], s=120, label=method,\n",
    "                color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(params[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Trainable Parameters (Millions)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Model Size vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "vjU0-NLCDbna",
    "outputId": "45b783be-adea-4608-d707-7933aa38d0b4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'results_print' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3070145270.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmethods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_print\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_print\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults_print\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_print' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "DEFAULT_R = 8\n",
    "DEFAULT_ALPHA = 16\n",
    "DEFAULT_DROPOUT = 0.1\n",
    "OPT_EPOCHS = 1\n",
    "\n",
    "def _run_trial_training(trial_method: str, lr: float, r: int = None, lora_alpha: int = None, lora_dropout: float = None):\n",
    "\n",
    "    # 1. Load Base Model\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "\n",
    "    if trial_method == \"LoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # 2. Setup Optimizer and Device\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # 3. Train\n",
    "    trained_model, _ = train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        trial_method,\n",
    "        train_loader,\n",
    "        DEVICE,\n",
    "        OPT_EPOCHS,\n",
    "        save_model=False\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate\n",
    "    eval_results = evaluate_model(\n",
    "        trained_model,\n",
    "        val_loader,\n",
    "        f1_metric,\n",
    "        THRESHOLD,\n",
    "        DEVICE,\n",
    "        trial_method\n",
    "    )\n",
    "\n",
    "    # 5. Cleanup\n",
    "    cleanup(trained_model, optimizer)\n",
    "\n",
    "    return eval_results[\"f1_macro\"]\n",
    "\n",
    "# STUDY 1: LoRA Learning Rate Optimization\n",
    "\n",
    "def objective_lora_lr(trial):\n",
    "\n",
    "    lr = trial.suggest_float('LEARNING_RATE_LoRA', 1e-6, 1e-4, log=True)\n",
    "\n",
    "    r = DEFAULT_R\n",
    "    lora_alpha = DEFAULT_ALPHA\n",
    "    lora_dropout = DEFAULT_DROPOUT\n",
    "\n",
    "    # Run the training\n",
    "    f1_macro = _run_trial_training(\n",
    "        trial_method='LoRA',\n",
    "        lr=lr,\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr('lora_r', r)\n",
    "    trial.set_user_attr('lora_alpha', lora_alpha)\n",
    "\n",
    "    return f1_macro\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STUDY 1: LoRA Learning Rate Optimization\")\n",
    "print(f\"Fixed Parameters: r={DEFAULT_R}, alpha={DEFAULT_ALPHA}, dropout={DEFAULT_DROPOUT}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_lora = optuna.create_study(direction='maximize', study_name=\"LoRA_LR_Study\")\n",
    "study_lora.optimize(objective_lora_lr, n_trials=25)\n",
    "\n",
    "print(\"Done Lora\")\n",
    "\n",
    "\n",
    "# STUDY 2: Full Fine-Tuning Learning Rate Optimization\n",
    "\n",
    "def objective_fullft_lr(trial):\n",
    "    lr = trial.suggest_float('FF_LEARNING_RATE_FullFT', 5e-7, 5e-6, log=True)\n",
    "\n",
    "    f1_macro = _run_trial_training(\n",
    "        trial_method='Full Fine-Tuning',\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    return f1_macro\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STUDY 2: Full Fine-Tuning Learning Rate Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_fullft = optuna.create_study(direction='maximize', study_name=\"FullFT_LR_Study\")\n",
    "study_fullft.optimize(objective_fullft_lr, n_trials=25)\n",
    "\n",
    "# LoRA Results\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"BEST LoRA LEARNING RATE RESULT\")\n",
    "print(\"#\"*50)\n",
    "print(f\"Best Macro F1: {study_lora.best_value:.4f}\")\n",
    "print(f\"Optimal Learning Rate: {study_lora.best_params['LEARNING_RATE_LoRA']:.2e}\")\n",
    "print(f\"Fixed Rank (r): {study_lora.best_trial.user_attrs['lora_r']}\")\n",
    "print(f\"Fixed Alpha: {study_lora.best_trial.user_attrs['lora_alpha']}\")\n",
    "print(\"#\"*50)\n",
    "\n",
    "# --- Full Fine Tuning\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"BEST FULL FINE-TUNING LEARNING RATE RESULT\")\n",
    "print(\"#\"*50)\n",
    "print(f\"Best Macro F1: {study_fullft.best_value:.4f}\")\n",
    "print(f\"Optimal Learning Rate: {study_fullft.best_params['FF_LEARNING_RATE_FullFT']:.2e}\")\n",
    "print(\"#\"*50)"
   ],
   "metadata": {
    "id": "Qz4Cttr67HMp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}