{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/main/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets accelerate evaluate pynvml\n",
    "!pip install -U peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip show peft transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBk7G5ubZBy1",
    "outputId": "dd1a7d5e-24c6-4b90-ee41-2a6905a83f69"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml) (12.575.51)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n",
      "Name: peft\n",
      "Version: 0.17.1\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AdaLoraConfig\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "NUM_LABELS = 28\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "FF_LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "# https://arxiv.org/pdf/2412.12148\n",
    "THRESHOLD = 0.5"
   ],
   "metadata": {
    "id": "_jAa-D4aZDe6"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "max_length = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    encodings['labels'] = batch['labels']\n",
    "    return encodings\n",
    "\n",
    "ds_encoded = ds.map(tokenize, batched=True)\n",
    "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
    "        if item['labels'] is not None:\n",
    "            for l in item['labels']:\n",
    "                if 0 <= l < NUM_LABELS:\n",
    "                    multi_hot[l] = 1.0\n",
    "        labels.append(multi_hot)\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluation Metric\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
   ],
   "metadata": {
    "id": "tbB9oI1PZGHc",
    "outputId": "12a47c4d-a4cc-48de-bc28-36664197deee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "cf8865b580004f1fa1338161293efc63",
      "77bfbe60ba144e31bdaa9f6ea77b6123",
      "fc45de5d3f994b8e9fc2e14c8123b1e4",
      "171b31c0d88145f4bfa7203e81053959",
      "7fc5f58e38054b7e843a7e6d2086f55c",
      "59cf265addba4062b6dc689fe7ab0ea4",
      "3204f2ae3b384a7b8c66001a72ed790a",
      "423c8b3b57a5481d80d1c0f9fc2ff4cb",
      "28426419aee94379925fe4258e65f338",
      "8e7c414e4330491ca66b89d73f756efc",
      "ed8f915921354c42bb8bf0e0c69a4b45",
      "b5f8956a026148b39157bb498e15f2ea",
      "69dbd598671841f38d0b7e76acc78e08",
      "9aab314247b04a73b2e8170eaf6d5ba7",
      "bbbc4fcd5c8a46e099834858ba58e0f7",
      "83cea0a0f012499cbac02b027331c894",
      "1300459e881543c7a5b607c287ae43b5",
      "8dd647111f9840bfa02336a551c28b7f",
      "589a5235f04345d99e060c18da78729e",
      "561f453ccad944a39ed0b8f8ddc746af",
      "b5d7729c378e4bd4afa125d21fbd9a23",
      "dc3774495dd040fdad748e022682504f",
      "453b1ec9ba4742b699f6cb0d6f5a0019",
      "401a5a1c1c8149e6bae6b997a4355648",
      "0bfc943d32694efebb8bc82e9c1bd176",
      "a9f71b8d306d47499a47ae1afec57f5d",
      "0369b501d1d74528b2d6eff0df4b090d",
      "1685989445ce4d5e9a6b800494d2c86d",
      "d5c994bc060646ad877665a2e31fe801",
      "813e3c0d460c4835b9152d4e3e47f24d",
      "63f157e801ae497e9a27b91f9c71895f",
      "2debef1356c542f49d760c9a0500e0f9",
      "bddb453eb3b94181aaab8d1757e7d8ee",
      "ea99b82630684881a85b51c87908a49a",
      "23d0deb6e8b7489ea4a5a882b68b93f2",
      "150ee528df884ae8a89956b8e1d9d2b8",
      "afd472c4bf4241ffbccd2366b246436b",
      "7854512470fa4d5f922f898ed75de621",
      "0187766e16774991a223cc9c3c3866da",
      "c4354479470b45ad9f02006a45cb6ae7",
      "8ef3ffeade324c9bbaa79b033d08047b",
      "ab5e77ad3d7645138cac3ca7a4043ed7",
      "db71be7ae91149eaa26eac7009e12e8b",
      "1fbc92e6ecf947b4bdc9d1922eef05f3",
      "46ffe721491d44ebbb8ce5515a0ff1a9",
      "2e0bb76066904b83a9d87f5de426e04a",
      "da978e5208ed4a25adcba73a7c7f94d7",
      "53cd58bf2a7847349512dd51fc092c60",
      "73198d6e104c4f59abcec0f8a35ff48d",
      "f4119a4693194a68a23814197ca8e9d6",
      "e55d3aa3738041b09e1ef7f9a2bb1965",
      "ba8c291009db43ef9193c730f1483e52",
      "c34186ee3eac48f29e5c246ffbcdc26e",
      "b2e3a9dc90fd4a7e97650e4f9f80ff60",
      "ec6cb6adb1bb46128496856531781bac",
      "b0e05d4909ac4b3fa08f4fb67b4094a9",
      "e2139b12af684fc3b72b3ad401e79800",
      "5341a023f80f444e8339e9029a8b84df",
      "63818eaa8c3f4cceb6a77cd8a4d08d7b",
      "595cfcdeabdf4bafa7dad428bb2680e2",
      "d350d5f3e82a4ce4afa6367ea6079ee6",
      "9f5192f7ed5e4d64b86d4a41e73f61a5",
      "0cb27f98c3d54079adb3e4083618d13a",
      "0a552803643d44b587248fb0d667dd35",
      "fbca0da8174848f19f90bebc1fc0e554",
      "651e6a67176e447d9171159359db944e",
      "297e91f83b154182a3efcfbbf1c31201",
      "93d76dff6cac49169122d9cc8a306aca",
      "716f1c8230df4344b1e4b0bfe651353b",
      "bcca15a3fb6f4fd99c249e2c074a404a",
      "df6b0eaf2b33459bbdc173d674924070",
      "d4354ec680b24d638055e3e38439216b",
      "f355037f61ab45d9ac06858d2597e298",
      "3ae22aa6fe87477dabad4ef5c2bdf110",
      "eb22b7d1546d49ba886e45503dae623f",
      "744c7dc34fc14f79ada890465068fd3f",
      "12f19c30a02a4f8ca2f39709954285c4",
      "2f1c4871973e4831a5b0412991b9819c",
      "ba56cca46b184e16ade9210bdebdc75f",
      "f69efb5e025a4f43a90af7c89db03a8e",
      "8f72bc57c17b4a48b67baa636abde491",
      "d1fb56d6d85341b18082930c5e18bee4",
      "93908ace49bf4e559a2413d41350ae94",
      "3263c14da5314859bb649af2da7ee00a",
      "28755cec950d43feace53946e1076fd8",
      "b44aa13adb1c4d1eb27ab2d8173fbdf1",
      "df76149cd9da40d29d6a6ed70b81ec44",
      "0eb77e0914d049058b23613de0bf6a81",
      "16558a33b8ce45e99de20bb7018f89a2",
      "8e6bda8d3b8646d8b99b31194b11b69a",
      "57f525fb0d13400f8e0576809aeefe64",
      "2363ffbae99e48b2a9dd4d2c260ed96a",
      "edd9729f375a4bda81e7b5724738366b",
      "9a13d89887d94ee6bef981340846417b",
      "3eb89017e7c64cb3805c1a8f5dd82795",
      "8b6c6894d17243f490a5265ee12ac1a5",
      "431d317d48da446087839287c79f0baf",
      "05066e82db4a4e27a9362368b2999e5d",
      "d51adc56165348b2b907eea978cbce34",
      "85e7af8974514c489af72f3b7e34c69d",
      "347bda0dc3d74b7a8d37febe4ef28f8f",
      "2a13fc14422c4414a1834164cdab014c",
      "01672d5ba9d74495838ce459705d807c",
      "fdf8230e03f041a2a46088aaadf976d0",
      "be3e66654ded4320862d46cb383b4d27",
      "fe6b09e7dfe5477492c50da0148a478e",
      "38238faab1264fcaa0bd29651b7432fd",
      "3d53628936e24d9b99a0c59fdc29e5b5",
      "027efcef1d814a95becbc1280b5bbdee",
      "6b3652767601402a932c14bf38077b8e",
      "19473e6e870f4762a497ca5caf6fde86",
      "2d2303aa9f9b43eab2f83b5db1a53ec9",
      "41910e5865b44c68a921c752e054b906",
      "cdbc7c875a7f4d8a961d80d22f823af0",
      "f04b8aeacba84f4ab65e50d4f3dc07c9",
      "c543827835a943629c67bc079e70ddb4",
      "e9b534e6361041cebcf3ad5695be72d9",
      "ac71c67866184af09653140d3e08e95a",
      "df26b1c7e7954955bc91a0e3373e5b6e",
      "b885332af7054d0e881535ec597f7350",
      "ebbc2a1e9156458a9ed0993ceac8ba8e",
      "82d64d59af2a47ca9c07d0138223b852",
      "3fd7a97b440746298e1644514ffaa6bd",
      "9161388773d842c5adfd5a42512c0289",
      "c7a75eaa72584530a9f606b673595105",
      "6ab107e703164eb28975bfd8b3093027",
      "db0de07f63c8443b8195267dfba6d10d",
      "5abec3adcdaf469a85319a25164dd552",
      "a9be83632e404d479d64c4a00de800d2",
      "3ee1c0fa5066414e8cf2994378c81555",
      "6ed290ec1ff8412f870975a240154330",
      "de9bbbe65c80443fb620be4964a267d6",
      "6902700c029a418bb5647efb8449d7ab",
      "b7f6599fdbb14b639841428bae838c78",
      "eacc4376870e486f8894c52a401f1feb",
      "48d6f3301d1a4b238c1234efd32a61de",
      "be8bbe407f934b77bb09f039bc54e560",
      "d06aefeb8bac4f88ac41e8ee29fc9cb4",
      "003dfc7430e94c7b8aceea958d0e6e3a",
      "779886bb621d4239abff28f41c15f119",
      "c90765c80b094330a861c526dab7af15",
      "b6e1fde956c942e08e67f7a1dcbf447e",
      "deaa85919eda488e885260c9eec1b95e",
      "4c86745b2eff490187740e975fb3a0dd",
      "d7292206cff94cb0b343356d13450f95",
      "48bea9e4c692414eb30a48a0ee052c5b",
      "8c66ec55a1744a179a5eb93474f94793",
      "f02542174e72405491d0f81daacdebc0",
      "ef7252528cdb4523b1ef3611fd389f5d",
      "a57ffdea9af348b18cc60b98a7554b72",
      "5371410aa57f49dbaa4fbb618bae84b0",
      "2dff0632d68b4fa68a7a832cc60c0ab0",
      "cff6da070c07467290c6037d6435bc53",
      "38e5a8dcc3e9412395d4dc27212f017e",
      "d640848467f6488eace298a06b76deb2",
      "f620ac36eb7b496888ce474d68a8d345",
      "608891e514e24d96a83a16241d89de8c",
      "c9722ba9ad9447e88b01afee43bfe10a",
      "1050d20daadf4cd59a48f26b124204ad",
      "3c5eea1d21a8499681bfbbab5d670b50",
      "a271d275614a43e9830c98b1d7c05a50",
      "7ac9a29ad3a94a8586e551dbb0cef054",
      "2b6cb5d539cf4c498b0493aa0f637085",
      "84ad55101b5842619f756389706477fc",
      "260f81b4e34744a0ac9c129541219152",
      "40071a6710bb4482b4c4664637a25f43",
      "f0b622510fe3437d8aebfeaf1b3de639",
      "b810a58788c645728e723ebc8425d841",
      "b1f15bfb024343eba221e78b81e31655",
      "e17372a7197e4043ae1a21d874b96a7f",
      "5c27a236eab84795a24148f60591380e",
      "b4af7ea141f6491993257205755b7de8",
      "c008b877d3374b2abe66ea0f75eb7217",
      "66714da88fdc4a7c8f596b7413541248",
      "5c37ea017bb944c79cbeb1b9adab850f",
      "26680f3d32144e0e9a4d60c1f3be6c55"
     ]
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf8865b580004f1fa1338161293efc63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5f8956a026148b39157bb498e15f2ea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/validation-00000-of-00001.par(\u2026):   0%|          | 0.00/350k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "453b1ec9ba4742b699f6cb0d6f5a0019"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea99b82630684881a85b51c87908a49a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46ffe721491d44ebbb8ce5515a0ff1a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0e05d4909ac4b3fa08f4fb67b4094a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "297e91f83b154182a3efcfbbf1c31201"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f1c4871973e4831a5b0412991b9819c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16558a33b8ce45e99de20bb7018f89a2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e7af8974514c489af72f3b7e34c69d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19473e6e870f4762a497ca5caf6fde86"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82d64d59af2a47ca9c07d0138223b852"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6902700c029a418bb5647efb8449d7ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c86745b2eff490187740e975fb3a0dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d640848467f6488eace298a06b76deb2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40071a6710bb4482b4c4664637a25f43"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_model(method: str):\n",
    "    \"\"\"\n",
    "    Prepares a RoBERTa model for fine-tuning using different PEFT strategies:\n",
    "    - \"Full\"      : full fine-tuning\n",
    "    - \"LoRA\"      : standard LoRA\n",
    "    - \"LoRA+\"     : LoRA with Rescaled Stable adaptation\n",
    "    - \"AdaLoRA\"   : Adaptive LoRA (dynamic rank allocation)\n",
    "    - \"DoRA\"      : Weight-decomposed LoRA (Meta 2024)\n",
    "    - \"QLoRA\"     : 4-bit quantized LoRA\n",
    "    \"\"\"\n",
    "\n",
    "    # Load Quantization Config\n",
    "    quantization_config = None\n",
    "    if method == \"QLoRA\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    # Load base RoBERTa\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\" if method == \"QLoRA\" else None,\n",
    "    )\n",
    "\n",
    "    # Choose PEFT variant\n",
    "    if method == \"LoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing LoRA\")\n",
    "\n",
    "    elif method == \"LoRA+\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_rslora=True,  # LoRA+\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\u2699\ufe0f Using LoRA+\")\n",
    "\n",
    "    elif method == \"AdaLoRA\":\n",
    "        config = AdaLoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            init_r=8,\n",
    "            target_r=4,\n",
    "            tinit=100,\n",
    "            tfinal=500,\n",
    "            deltaT=10,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            total_step=len(train_loader) * EPOCHS\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing AdaLoRA\")\n",
    "\n",
    "    elif method == \"DoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_dora=True,  # enables DoRA\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing DoRA\")\n",
    "\n",
    "    elif method == \"QLoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\nUsing QLoRA (4-bit quantized + LoRA)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nFull Fine-Tuning (no adapters)\")\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100%)\")\n",
    "        model.to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
    "        return model, optimizer\n",
    "\n",
    "    # Shared setup for PEFT variants\n",
    "    model.print_trainable_parameters()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    return model, optimizer\n"
   ],
   "metadata": {
    "id": "1jfdNdbbZHvC"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, optimizer, method: str, train_loader, device, epochs, save_model):\n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    gpu_utilization_history = []\n",
    "    max_gpu_mem_allocated = 0.0\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(device.index if device.index is not None else 0)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if device.type == 'cuda':\n",
    "                util = nvmlDeviceGetUtilizationRates(handle)\n",
    "                gpu_util_percent = util.gpu\n",
    "                gpu_utilization_history.append(gpu_util_percent)\n",
    "\n",
    "                allocated_mem_bytes = torch.cuda.memory_allocated(device)\n",
    "\n",
    "                max_gpu_mem_allocated = max(max_gpu_mem_allocated, allocated_mem_bytes / (1024**3))\n",
    "\n",
    "                loop.set_postfix(\n",
    "                    loss=loss.item(),\n",
    "                    gpu_util_perc=f\"{gpu_util_percent}%\",\n",
    "                    gpu_mem_gb=f\"{allocated_mem_bytes / (1024**3):.2f}\"\n",
    "                )\n",
    "            else:\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    total_train_time = time.time() - start_time\n",
    "    results['train_time_sec'] = total_train_time\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        results['max_gpu_mem_gb'] = max_gpu_mem_allocated\n",
    "        results['max_gpu_percent'] = max(gpu_utilization_history) if gpu_utilization_history else 0\n",
    "        results['average_gpu_percent'] = sum(gpu_utilization_history) / len(gpu_utilization_history) if gpu_utilization_history else 0\n",
    "\n",
    "        nvmlShutdown()\n",
    "\n",
    "\n",
    "    if save_model:\n",
    "      cpt_str = method + \"_checkpoint.pth\"\n",
    "      torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, cpt_str)\n",
    "\n",
    "    return model, results"
   ],
   "metadata": {
    "id": "Kv5EoKDDcwcN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    hamming_loss,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred_probs = torch.sigmoid(logits)\n",
    "            pred_labels = (pred_probs > threshold).long()\n",
    "\n",
    "            all_probs.append(pred_probs.cpu().numpy())\n",
    "            all_preds.append(pred_labels.cpu().numpy())\n",
    "            all_targets.append(labels.long().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    probs = np.concatenate(all_probs, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Multi-label metrics\n",
    "    results = {\n",
    "        \"f1_macro\": f1_score(targets, preds, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(targets, preds, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(targets, preds, average=\"weighted\"),\n",
    "        \"precision_macro\": precision_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"hamming_loss\": hamming_loss(targets, preds),\n",
    "        \"exact_match_accuracy\": np.mean([np.all(p == t) for p, t in zip(preds, targets)]),\n",
    "    }\n",
    "\n",
    "    # Probabilistic metrics (optional)\n",
    "    try:\n",
    "        results[\"roc_auc_macro\"] = roc_auc_score(targets, probs, average=\"macro\")\n",
    "        results[\"pr_auc_macro\"] = average_precision_score(targets, probs, average=\"macro\")\n",
    "    except ValueError:\n",
    "        results[\"roc_auc_macro\"] = None\n",
    "        results[\"pr_auc_macro\"] = None\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"| {method} Evaluation Results |\")\n",
    "    print(\"-\" * 50)\n",
    "    for k, v in results.items():\n",
    "        if v is not None:\n",
    "            print(f\"{k.replace('_',' ').title():30}: {v:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "id": "YMmQhmOoe9Nh"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cleanup(full_model=None, full_optimizer=None):\n",
    "    if full_model is not None:\n",
    "        del full_model\n",
    "    if full_optimizer is not None:\n",
    "        del full_optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "RSq1mQm1sLod"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_table = []\n",
    "\n",
    "print(\"Before FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating model before training...\")\n",
    "pretrain_eval = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    \"Full Fine-Tuning (Before Training)\"\n",
    ")\n",
    "\n",
    "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
    "results_table.append({\"Method\": \"Before Training\", **before_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247,
     "referenced_widgets": [
      "4ba0c4a8f101486ba45e966cc71ee06d",
      "7baf383f00f14b90a001e7db196e3e97",
      "2ae1b0a9f78645daaa81e9f43217b043",
      "c365785bedc6435f84cf2ba3a3d15cf3",
      "a5180cf5c7b74d709e5870aeede13e82",
      "de6895091cf844f2badc6d61dca01f35",
      "6fbe1a2e85614d3ba7e4de4ecdb11555",
      "a30adbe53b4a4470a024acf22e89c139",
      "dc5abbca6c9042b0bd40336deb64a3f4",
      "c2365e17c45d4749a03f341e819c14ab",
      "50f85e58b6d7439a8041a5d6fed13cad"
     ]
    },
    "id": "24TkGpNProMd",
    "outputId": "111c516c-107c-4155-d2ed-c1bc771dd6a4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before FINE-TUNING\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ba0c4a8f101486ba45e966cc71ee06d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Full Fine-Tuning (no adapters)\n",
      "Total Parameters: 124.67M\n",
      "Trainable Parameters: 124.67M (100%)\n",
      "\n",
      "Evaluating model before training...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "full_method = \"Full Fine-Tuning\"\n",
    "\n",
    "full_model, full_optimizer = prepare_model(full_method)\n",
    "\n",
    "train_full_model = True\n",
    "\n",
    "if (train_full_model):\n",
    "  # Train\n",
    "  full_model, full_train_results = train_model(\n",
    "      full_model,\n",
    "      full_optimizer,\n",
    "      full_method,\n",
    "      train_loader,\n",
    "      DEVICE,\n",
    "      EPOCHS,\n",
    "      save_model=True\n",
    "  )\n",
    "else:\n",
    "  # Load checkpoint\n",
    "  cpt_string = full_method + \"_checkpoint.pth\"\n",
    "  checkpoint = torch.load(cpt_string)\n",
    "  full_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  full_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  results = {}\n",
    "  results['train_time_sec'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "full_eval_results = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    full_method\n",
    ")\n",
    "\n",
    "full_results = {**full_train_results, **full_eval_results}\n",
    "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)"
   ],
   "metadata": {
    "id": "nOh-mU-ycSjn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del lora_model\n",
    "del lora_optimizer\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "eZAoWflrXvFh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# MULTI-RUN LoRA / QLoRA / AdaLoRA / DoRA BENCHMARK\n",
    "# ============================================================\n",
    "# import torch\n",
    "\n",
    "methods_to_run = [\"DoRA\"]\n",
    "# results_table = []\n",
    "\n",
    "for lora_method in methods_to_run:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Starting Fine-Tuning with {lora_method}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    train_lora_model = True\n",
    "\n",
    "    # ---- Prepare model and optimizer ----\n",
    "    try:\n",
    "        lora_model, lora_optimizer = prepare_model(lora_method)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to prepare {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if train_lora_model:\n",
    "        # ---- Train ----\n",
    "        try:\n",
    "            lora_model, lora_train_results = train_model(\n",
    "                lora_model,\n",
    "                lora_optimizer,\n",
    "                lora_method,\n",
    "                train_loader,\n",
    "                DEVICE,\n",
    "                EPOCHS,\n",
    "                save_model=True\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"Skipping {lora_method} (Out of memory)\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed for {lora_method}: {e}\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        # ---- Load checkpoint ----\n",
    "        cpt_string = f\"{lora_method}_checkpoint.pth\"\n",
    "        checkpoint = torch.load(cpt_string)\n",
    "        lora_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lora_optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        loss = checkpoint[\"loss\"]\n",
    "        lora_train_results = {\"train_time_sec\": 0, \"final_loss\": loss}\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    try:\n",
    "        lora_eval_results = evaluate_model(\n",
    "            lora_model,\n",
    "            val_loader,\n",
    "            f1_metric,\n",
    "            THRESHOLD,\n",
    "            DEVICE,\n",
    "            lora_method\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed for {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ---- Merge and store results ----\n",
    "    lora_results = {**lora_train_results, **lora_eval_results}\n",
    "    results_table.append({\"Method\": lora_method, **lora_results})\n",
    "\n",
    "    # ---- Cleanup GPU memory ----\n",
    "    del lora_model\n",
    "    del lora_optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to DataFrame for nice display (optional)\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results_table)\n",
    "display(results_df)\n"
   ],
   "metadata": {
    "id": "4EfsUIe4rwN9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\\nCOMPARISON OF RESULTS\")\n",
    "print(\"#\" * 100)\n",
    "\n",
    "# Print table header with new metrics\n",
    "print(f\"| {'Method':<20} | {'Trainable Params (M)':<20} | {'Train Time (s)':<15} | \"\n",
    "      f\"{'Macro F1':<10} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} |\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Print each row from results_table\n",
    "for result in results_table:\n",
    "    method_name = result[\"Method\"]\n",
    "    trainable_params = result.get(\"trainable_params\", 0.0)\n",
    "    train_time = result.get(\"train_time_sec\", 0.0)\n",
    "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
    "    accuracy = result.get(\"accuracy\", 0.0)\n",
    "    precision = result.get(\"precision_macro\", 0.0)\n",
    "    recall = result.get(\"recall_macro\", 0.0)\n",
    "\n",
    "    print(f\"| {method_name:<20} | {trainable_params / 1e6:<20.4f} | {train_time:<15.2f} | \"\n",
    "          f\"{f1_macro:<10.4f} | {accuracy:<10.4f} | {precision:<10.4f} | {recall:<10.4f} |\")\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n"
   ],
   "metadata": {
    "id": "5ciWQIO_rzhl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract plotting data from results_table\n",
    "methods = [r[\"Method\"] for r in results_table]\n",
    "params = [r.get(\"trainable_params\", 0.0) / 1e6 for r in results_table]  # in millions\n",
    "train_times = [r.get(\"train_time_sec\", 0.0) for r in results_table]\n",
    "f1_macros = [r.get(\"f1_macro\", 0.0) for r in results_table]\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "#  Plot 1: Training Time vs F1\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(train_times[i], f1_macros[i], s=120, label=method,\n",
    "                color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(train_times[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Training Time (seconds)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Training Time vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Parameters vs F1\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(params[i], f1_macros[i], s=120, label=method,\n",
    "                color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(params[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Trainable Parameters (Millions)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Model Size vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# ====== 3. ROC AUC vs Training Time ======\n",
    "plt.figure(figsize=(6,5))\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(train_times[i], roc_aucs[i], s=120, color=colors[i % len(colors)],\n",
    "                edgecolors='black', linewidth=1.2)\n",
    "    plt.text(train_times[i]*1.01, roc_aucs[i], method, fontsize=9, va='center')\n",
    "plt.xlabel(\"Training Time (seconds)\", fontsize=11)\n",
    "plt.ylabel(\"ROC AUC (Macro)\", fontsize=11)\n",
    "plt.title(\"Training Time vs ROC AUC\", fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# ====== 4. PR AUC vs F1 Macro ======\n",
    "plt.figure(figsize=(6,5))\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(pr_aucs[i], f1_macros[i], s=120, color=colors[i % len(colors)],\n",
    "                edgecolors='black', linewidth=1.2)\n",
    "    plt.text(pr_aucs[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel(\"PR AUC (Macro)\", fontsize=11)\n",
    "plt.ylabel(\"Macro F1 Score\", fontsize=11)\n",
    "plt.title(\"PR AUC vs F1 Macro\", fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# ====== 5. Hamming Loss vs F1 ======\n",
    "plt.figure(figsize=(6,5))\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(hamming_losses[i], f1_macros[i], s=120, color=colors[i % len(colors)],\n",
    "                edgecolors='black', linewidth=1.2)\n",
    "    plt.text(hamming_losses[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel(\"Hamming Loss\", fontsize=11)\n",
    "plt.ylabel(\"Macro F1 Score\", fontsize=11)\n",
    "plt.title(\"Error vs Performance (Hamming Loss vs F1)\", fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# ====== 6. Radar Chart (Multi-Metric Comparison) ======\n",
    "metrics = [\"f1_macro\", \"precision_macro\", \"recall_macro\", \"roc_auc_macro\", \"pr_auc_macro\"]\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "vjU0-NLCDbna"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DEFAULT_R = 8\n",
    "DEFAULT_ALPHA = 16\n",
    "DEFAULT_DROPOUT = 0.1\n",
    "OPT_EPOCHS = 1\n",
    "\n",
    "def _run_trial_training(trial_method: str, lr: float, r: int = None, lora_alpha: int = None, lora_dropout: float = None):\n",
    "\n",
    "    # 1. Load Base Model\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "\n",
    "    if trial_method == \"LoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # 2. Setup Optimizer and Device\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # 3. Train\n",
    "    trained_model, _ = train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        trial_method,\n",
    "        train_loader,\n",
    "        DEVICE,\n",
    "        OPT_EPOCHS,\n",
    "        save_model=False\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate\n",
    "    eval_results = evaluate_model(\n",
    "        trained_model,\n",
    "        val_loader,\n",
    "        f1_metric,\n",
    "        THRESHOLD,\n",
    "        DEVICE,\n",
    "        trial_method\n",
    "    )\n",
    "\n",
    "    # 5. Cleanup\n",
    "    cleanup(trained_model, optimizer)\n",
    "\n",
    "    return eval_results[\"f1_macro\"]\n",
    "\n",
    "# STUDY 1: LoRA Learning Rate Optimization\n",
    "\n",
    "def objective_lora_lr(trial):\n",
    "\n",
    "    lr = trial.suggest_float('LEARNING_RATE_LoRA', 1e-6, 1e-4, log=True)\n",
    "\n",
    "    r = DEFAULT_R\n",
    "    lora_alpha = DEFAULT_ALPHA\n",
    "    lora_dropout = DEFAULT_DROPOUT\n",
    "\n",
    "    # Run the training\n",
    "    f1_macro = _run_trial_training(\n",
    "        trial_method='LoRA',\n",
    "        lr=lr,\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr('lora_r', r)\n",
    "    trial.set_user_attr('lora_alpha', lora_alpha)\n",
    "\n",
    "    return f1_macro\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STUDY 1: LoRA Learning Rate Optimization\")\n",
    "print(f\"Fixed Parameters: r={DEFAULT_R}, alpha={DEFAULT_ALPHA}, dropout={DEFAULT_DROPOUT}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_lora = optuna.create_study(direction='maximize', study_name=\"LoRA_LR_Study\")\n",
    "study_lora.optimize(objective_lora_lr, n_trials=25)\n",
    "\n",
    "print(\"Done Lora\")\n",
    "\n",
    "\n",
    "# STUDY 2: Full Fine-Tuning Learning Rate Optimization\n",
    "\n",
    "def objective_fullft_lr(trial):\n",
    "    lr = trial.suggest_float('FF_LEARNING_RATE_FullFT', 5e-7, 5e-6, log=True)\n",
    "\n",
    "    f1_macro = _run_trial_training(\n",
    "        trial_method='Full Fine-Tuning',\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    return f1_macro\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STUDY 2: Full Fine-Tuning Learning Rate Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study_fullft = optuna.create_study(direction='maximize', study_name=\"FullFT_LR_Study\")\n",
    "study_fullft.optimize(objective_fullft_lr, n_trials=25)\n",
    "\n",
    "# LoRA Results\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"BEST LoRA LEARNING RATE RESULT\")\n",
    "print(\"#\"*50)\n",
    "print(f\"Best Macro F1: {study_lora.best_value:.4f}\")\n",
    "print(f\"Optimal Learning Rate: {study_lora.best_params['LEARNING_RATE_LoRA']:.2e}\")\n",
    "print(f\"Fixed Rank (r): {study_lora.best_trial.user_attrs['lora_r']}\")\n",
    "print(f\"Fixed Alpha: {study_lora.best_trial.user_attrs['lora_alpha']}\")\n",
    "print(\"#\"*50)\n",
    "\n",
    "# --- Full Fine Tuning\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"BEST FULL FINE-TUNING LEARNING RATE RESULT\")\n",
    "print(\"#\"*50)\n",
    "print(f\"Best Macro F1: {study_fullft.best_value:.4f}\")\n",
    "print(f\"Optimal Learning Rate: {study_fullft.best_params['FF_LEARNING_RATE_FullFT']:.2e}\")\n",
    "print(\"#\"*50)"
   ],
   "metadata": {
    "id": "Qz4Cttr67HMp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}