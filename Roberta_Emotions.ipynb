{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/QLora/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install necessary libraries (PEFT is the modern standard for LoRA)\n",
    "!pip install transformers datasets accelerate evaluate\n",
    "!pip install -U peft\n",
    "!pip install -U bitsandbytes\n",
    "!pip show peft transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBk7G5ubZBy1",
    "outputId": "1e177ddf-20e3-4292-e1b8-7f4db148e3a5"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n",
      "Name: peft\n",
      "Version: 0.17.1\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.57.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, AdaLoraConfig\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "NUM_LABELS = 28\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "FF_LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "# https://arxiv.org/pdf/2412.12148\n",
    "THRESHOLD = 0.5"
   ],
   "metadata": {
    "id": "_jAa-D4aZDe6"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "max_length = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    encodings['labels'] = batch['labels']\n",
    "    return encodings\n",
    "\n",
    "ds_encoded = ds.map(tokenize, batched=True)\n",
    "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
    "        if item['labels'] is not None:\n",
    "            for l in item['labels']:\n",
    "                if 0 <= l < NUM_LABELS:\n",
    "                    multi_hot[l] = 1.0\n",
    "        labels.append(multi_hot)\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluation Metric\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
   ],
   "metadata": {
    "id": "tbB9oI1PZGHc",
    "outputId": "5b125e24-f74f-4cf9-a350-bb2b7da7723b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "c7a9463ff47b4bd7ac3904ed9fca0a28",
      "09dc305bae1b4dfaaa22828f6e586e2c",
      "791f2ec7b9984b7a8fc835bb1b01d132",
      "4c0d776aab5f4523be260d85442028a1",
      "001b6ea0f470431cbb31daa5c91f3bfb",
      "b3a33100418244b18ef113eb33f34519",
      "0fe3639f38e847d5958647cfc3d2c572",
      "b38dc4fad16549b5ab8f33478538760a",
      "965fd3b1b43a46e69fc0cf423622f724",
      "31e41ffd7d7f4ff0a60ca9db5e5ab817",
      "908c21d890504acdaf4c767fd5efcce8",
      "2a6dc9ae5924463cbf7d09168ec9fbc0",
      "4fcac30e3d5c45be9132fdaafbb3a9ad",
      "6ea21300117d4ee0b1a5e7f067b66ca6",
      "8eb4a601bbbc481ca08d77e97b842086",
      "2e5e8cedbd55403bae532af585aa8790",
      "272173fdc71a4004af44a0a58e413d80",
      "916866c2f49647b6a3513ec4d8d2b560",
      "9e1bf2172ab84a5194feb24b3a5e39c4",
      "9c5a0f6079014c76996a1c06d2c0f5fc",
      "582d5ce212c64ba58ede682a3afef367",
      "ffc01e78bf3246b7a30fc3c4efa8476a",
      "e1be291ce2334357ba691ace1d582a7b",
      "78398bc5549a44da9fedb36dfaa918c8",
      "c0016c7abc26470b8d91daf7550f0215",
      "21225ed9e49a4ebcb2f5f6c6eed6bfc4",
      "4b3fb8245ff249499d60440e0227f234",
      "24db68fc4fbf4261aa8b9a61f411aad6",
      "8893fc3b6bbd47619862f54322eedce2",
      "d4d4ac99b8144b3089de2ae23fa01d91",
      "9960fe47fec5452caea21e549e5b9531",
      "b0538bb3efc54d6791375325fdae3619",
      "7e37c082edba4b4f9530046c5d3c95ca",
      "dc52bb34353f4855b3f85423acca31c1",
      "6a4cd11a546f4952b4442b9f6a368e74",
      "ebb19a9604564063a6302d834621d2f0",
      "249440b23c574cef91bafcb753a8f96e",
      "dd4efdb5321d42eca86c5377fb5ea4c7",
      "ede6ca7de7bd4ab8a9c1034139c55611",
      "36f96e8f3de3418787dfb391e5a6d579",
      "1bba4529438f4b48a0bc29f1a7751e73",
      "539d819f5e844e579329c42d31be8347",
      "c54cd1c857994804930d1eba56b061f1",
      "e4273ccde2b446f9bcd28926a16d69b2",
      "9ddb59ae82dd4f39b642fa6e237ff05a",
      "aa2eba6950d740ccb34e1fc38713465d",
      "705fec23499c444b969ce73b115f1fb9",
      "1ab4292a5ccb44ea8ef60f92b4dc4b41",
      "b3545dd3c39e42a5adfe6354d1ce2837",
      "52fb27942943410dab70a4f72f1acdda",
      "263cec8e01244409b3cdf8c6067fc6ca",
      "7349a92993754e858704d8c304f53dbc",
      "3bd9da94e10e4f56a3379d58faacad9f",
      "4b5176ba75aa49caa2c35dd9ece10cf6",
      "3cd32bc17f5d446db99ef5588e206f01",
      "420accb56d53415cba9cd0f94e9e5b3b",
      "46fae28dda3f47d582264ff72a3da933",
      "c7e28701ea904bb2b2b4470b38611b97",
      "0befaaedef244506ab8806a70b7119f2",
      "15f14e930ee54b938cf2e51cff2e73a5",
      "5e388576065e43b193a9151fbd1a8387",
      "c8d6a63aaf2f4a24ab23ecaab527c2ff",
      "143bd8ee369342eb8db1fd532ba208bd",
      "a8c94268798b4da4856d089904f7d2f3",
      "89fae338b2b04371b5d4196e7d318ffc",
      "d7d5274a2ecd4fda8d36fb253ff865de",
      "0305946a0e694ae68e3764e644e8a908",
      "ca343b13c71843e99d051193c99d816e",
      "0ecc41926775464b86cbe4d98869047c",
      "e8c641879a5543c0a106227e041449f0",
      "b75a778f90304b68bfc09d166333aadd",
      "17055fdb061f4348b7c58123eeb12995",
      "475dfd15df1f4b14a2c00ff5d008f0de",
      "f8f8a50f3d434f409be4116568b76aaf",
      "d2cc216fb13648468d397db42168681c",
      "c848593cf6f6463a880cda28ab372155",
      "cd4e6507caa944ee8f0c8f2a1f968d08",
      "3acf20681ea64063bbb4e533f49b57d4",
      "d588e77fe13643da9f60857b028a1863",
      "4da7c37d3a3a4d95807c07685ab167a5",
      "fc7b98e62334472089ba7ce230af8ca8",
      "7e0280b958cd4ad3bcbbc9cc25c566a8",
      "0cfd23183dc14b708ca33b00dd663011",
      "d0ac917ecb434af88d67b9d0ecf052ff",
      "42088edf510e413d95f7892dea0e0751",
      "15fe30b689d24537967e50c7a640510d",
      "4d287bfc67d744088ae57de84ee361c9",
      "315ab2770dab4c09bbee427d70eae25b",
      "a27cc840ea9c49539c1438dcb38efb5a",
      "9f6493a17cea49e981a6f0f227c18155",
      "03a6099f3e5943f99ca6ccc168f21ff5",
      "a2517f3260324950a182d8092379d76c",
      "8d5464b7c3f349c684eecff0cdd01f8b",
      "4d05ec92527e4aeca079428425e0c236",
      "325506a8357e4e95bafbec7b70c220b1",
      "3c833736b2f24e3a82e6795f40b9260d",
      "e9655883e830468185e218d0c00565ce",
      "380545ff9a914a72b6ea67c3aaca520b",
      "17a742cb77274893a2e96c12e15ee547",
      "7fc8e82b07b1406296810a0c24956907",
      "5688a5ab9fa342c8b774778de5a313a6",
      "d18c6cc1be0e48ec8e91edc8a317df87",
      "fbd59a18d7854b7d914de8b4f733e1b3",
      "55d9002cfa9a4f8c9c5a9f91911b6e63",
      "d370e6a55fb746b9ab8c3afec4ff447b",
      "40612979d65e4b438ec58271f9207039",
      "1719686bf1e849b6bcca0af48ccafbf2",
      "32b4526e4f0448d1bca50da890bb07ba",
      "2ceeaeab5e9f4b5699280ec534ebb9d1",
      "5d0ddd44316a4cc6a5f7918b8a292bc2",
      "689a2fb5e4814f13808676901ee55275",
      "5fb01633ac5f4e14a156cae26025a735",
      "f51d26cde25f46569b610454c32f1734",
      "a7add3c8a67c4f839a55f7cbdd1b7e59",
      "bf920bacb78e42db87d8e5b3fcf612b1",
      "2a076688645c4ffb9a014a36667c90a2",
      "98ee36b30e2947c1b2f98c2825f078c2",
      "f4aa961073864a629cdb22f09c2fd0ce",
      "21e3ecdd7bb74a82a6dd24ac30c8c393",
      "a56653b8dd1048fea49e9fb3927265fb",
      "bf91fc469d834d47a6075f837d8cff3d",
      "1a161835b7554996b9a6fbe63c3272ae",
      "8ff546b3cf5844b890e6141fe42281ba",
      "d6394de118e84c909040a3f48a3caa4e",
      "f6ea537d86ac48c28247ed96c56c95b1",
      "fdde3c4dab3b4dc384cda76189eb936a",
      "995da27f4fe44d86a5a219fe0a203d4c",
      "cde5e004c91e440782e35f536fe0d5ea",
      "32cc6e0cbb6f4a21ba347359f34215ac",
      "9e5e4521883347008a2c4cfd65524ef9",
      "e0559963ebb945f5bdc5bcc7f4ad0a1d",
      "3c1b7961b3614ef99fe870efdea380bd",
      "c5c83ccc64c14abaa44226ba580ef7e4",
      "5e7a50bc26fd4a79ba9269a6020945ea",
      "ade7f8a6f45740e7b7f746a0b8457b9e",
      "9ba6e77d34074da7ae607d36a194c1f6",
      "1cc708fb5f7343808d7b25c452494887",
      "892ae92e44794855a7126baac0918a2d",
      "57666b656f4143489ce9514409f091d7",
      "9d41aafaef984e60bda9b5c785f11478",
      "d3fb1c89dfcf4c779ac42bbc9033b31e",
      "f800a0c617d9448db84822c29e3c3ac6",
      "c2b2eba3839349f7ad1441689f287426",
      "a8b6b973468440039b0a7cbac9481bf6",
      "a7ba9ef47eff42df9ec1cd4471d14b73",
      "dbbd99723b48434691cd8940228b1e5a",
      "7648cf4d434d4504b006108b83a3153a",
      "309206c6c3ba4063a21ba7e4ae39ae44",
      "0282da3c4fe64fb6a4d3d11b1d5ade0f",
      "6aac158bd52141e2aa324c58de3da66b",
      "2083f653e3114c07ab192b3b000540f6",
      "c92ccd3e85c94bb092692779959de605",
      "a3f558c2bfb14d9191ce9e5039a22b3f",
      "cec0d1276d2d43398e48baec5eb1d74c",
      "7b72f057540446418e0c99594081c294",
      "2142be6723174ea8bd39c2b343096d2d",
      "3dcfe571315b43d5aeb265ad98309daa",
      "65676c98a3444270a4a912af436ffd20",
      "c0ef4af2cd6c4ae39f0eb8bebf4e6f82",
      "8ff9e68f76284da1992944fcbfc54350",
      "9c7f6be99b61443882197ca86917f4f7",
      "6552b51228b74e09b44ac29fe413521a",
      "c5591c3d51ec4224b88d89adf278976d",
      "341b144781f94a7b982aac192db2aa27",
      "4875a02da1cc4ca185760baa9f8d0452",
      "f894ef711a9745f6a8d1b007283ca9bd",
      "1af06e1845a14ce2a71c256e93a47257",
      "81d804b94e0943cb82bddaba01b0b2f8",
      "2ea53b746b6f49daa73aa6f884e75cd4",
      "ee2d2616062648a498b6549039515638",
      "0aa15597b91240e4934ca89b2acce769",
      "e01a446a84024fde9166830b38d9f2fc",
      "111131e674244fcfa561afc67f1a168b",
      "248ae87a3e324dc28f235c63a0843277",
      "c02a1ce8d5e643928234a13ba742f101",
      "2d66223eff1344b8996b30f80e2f6d0c"
     ]
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7a9463ff47b4bd7ac3904ed9fca0a28"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a6dc9ae5924463cbf7d09168ec9fbc0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/validation-00000-of-00001.par(\u2026):   0%|          | 0.00/350k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1be291ce2334357ba691ace1d582a7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc52bb34353f4855b3f85423acca31c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ddb59ae82dd4f39b642fa6e237ff05a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "420accb56d53415cba9cd0f94e9e5b3b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0305946a0e694ae68e3764e644e8a908"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3acf20681ea64063bbb4e533f49b57d4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27cc840ea9c49539c1438dcb38efb5a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fc8e82b07b1406296810a0c24956907"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "689a2fb5e4814f13808676901ee55275"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a161835b7554996b9a6fbe63c3272ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5c83ccc64c14abaa44226ba580ef7e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8b6b973468440039b0a7cbac9481bf6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b72f057540446418e0c99594081c294"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f894ef711a9745f6a8d1b007283ca9bd"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_model(method: str):\n",
    "    \"\"\"\n",
    "    Prepares a RoBERTa model for fine-tuning using different PEFT strategies:\n",
    "    - \"Full\"      : full fine-tuning\n",
    "    - \"LoRA\"      : standard LoRA\n",
    "    - \"LoRA+\"     : LoRA with Rescaled Stable adaptation\n",
    "    - \"AdaLoRA\"   : Adaptive LoRA (dynamic rank allocation)\n",
    "    - \"DoRA\"      : Weight-decomposed LoRA (Meta 2024)\n",
    "    - \"QLoRA\"     : 4-bit quantized LoRA\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Optional quantization ----------\n",
    "    quantization_config = None\n",
    "    if method == \"QLoRA\":\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    # ---------- Load base RoBERTa ----------\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\" if method == \"QLoRA\" else None,\n",
    "    )\n",
    "\n",
    "    # ---------- Choose PEFT variant ----------\n",
    "    if method == \"LoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\ud83e\udde9 Using LoRA\")\n",
    "\n",
    "    elif method == \"LoRA+\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_rslora=True,  # LoRA+\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\u2699\ufe0f Using LoRA+\")\n",
    "\n",
    "    elif method == \"AdaLoRA\":\n",
    "        config = AdaLoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            init_r=8,\n",
    "            target_r=4,\n",
    "            tinit=100,\n",
    "            tfinal=500,\n",
    "            deltaT=10,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            total_step=len(train_loader) * EPOCHS\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\ud83d\ude80 Using AdaLoRA\")\n",
    "\n",
    "    elif method == \"DoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            use_dora=True,  # enables DoRA\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\ud83e\udde0 Using DoRA\")\n",
    "\n",
    "    elif method == \"QLoRA\":\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        print(\"\\n\ud83d\udcbe Using QLoRA (4-bit quantized + LoRA)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n\ud83e\uddf1 Full Fine-Tuning (no adapters)\")\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100%)\")\n",
    "        model.to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
    "        return model, optimizer\n",
    "\n",
    "    # ---------- Shared setup for PEFT variants ----------\n",
    "    model.print_trainable_parameters()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    return model, optimizer\n"
   ],
   "metadata": {
    "id": "1jfdNdbbZHvC"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, optimizer, method: str, train_loader, device, epochs, save_model):\n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    total_train_time = time.time() - start_time\n",
    "    results['train_time_sec'] = total_train_time\n",
    "\n",
    "    cpt_str = method + \"_checkpoint.pth\"\n",
    "\n",
    "    if save_model:\n",
    "      torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, cpt_str)\n",
    "\n",
    "    return model, results"
   ],
   "metadata": {
    "id": "Kv5EoKDDcwcN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    hamming_loss,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred_probs = torch.sigmoid(logits)\n",
    "            pred_labels = (pred_probs > threshold).long()\n",
    "\n",
    "            all_probs.append(pred_probs.cpu().numpy())\n",
    "            all_preds.append(pred_labels.cpu().numpy())\n",
    "            all_targets.append(labels.long().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    probs = np.concatenate(all_probs, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # --- Multi-label metrics ---\n",
    "    results = {\n",
    "        \"f1_macro\": f1_score(targets, preds, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(targets, preds, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(targets, preds, average=\"weighted\"),\n",
    "        \"precision_macro\": precision_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(targets, preds, average=\"macro\", zero_division=0),\n",
    "        \"hamming_loss\": hamming_loss(targets, preds),\n",
    "        \"exact_match_accuracy\": np.mean([np.all(p == t) for p, t in zip(preds, targets)]),\n",
    "    }\n",
    "\n",
    "    # --- Probabilistic metrics (optional) ---\n",
    "    try:\n",
    "        results[\"roc_auc_macro\"] = roc_auc_score(targets, probs, average=\"macro\")\n",
    "        results[\"pr_auc_macro\"] = average_precision_score(targets, probs, average=\"macro\")\n",
    "    except ValueError:\n",
    "        results[\"roc_auc_macro\"] = None\n",
    "        results[\"pr_auc_macro\"] = None\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"| {method} Evaluation Results |\")\n",
    "    print(\"-\" * 50)\n",
    "    for k, v in results.items():\n",
    "        if v is not None:\n",
    "            print(f\"{k.replace('_',' ').title():30}: {v:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "id": "YMmQhmOoe9Nh"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cleanup(full_model=None, full_optimizer=None):\n",
    "    if full_model is not None:\n",
    "        del full_model\n",
    "    if full_optimizer is not None:\n",
    "        del full_optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "RSq1mQm1sLod"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_table = []\n",
    "\n",
    "print(\"Before FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating model before training...\")\n",
    "pretrain_eval = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    \"Full Fine-Tuning (Before Training)\"\n",
    ")\n",
    "\n",
    "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
    "results_table.append({\"Method\": \"Before Training\", **before_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24TkGpNProMd",
    "outputId": "bbed6c16-d439-484f-9ba8-8ab8c87fd194"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before FINE-TUNING\n",
      "============================================================\n",
      "\n",
      "\ud83e\uddf1 Full Fine-Tuning (no adapters)\n",
      "Total Parameters: 124.67M\n",
      "Trainable Parameters: 124.67M (100%)\n",
      "\n",
      "Evaluating model before training...\n",
      "--------------------------------------------------\n",
      "| Full Fine-Tuning (Before Training) Evaluation Results |\n",
      "--------------------------------------------------\n",
      "Validation Macro F1 Score: 0.0578\n",
      "Validation Exact Match Accuracy: 0.0000\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "full_method = \"Full Fine-Tuning\"\n",
    "\n",
    "full_model, full_optimizer = prepare_model(full_method)\n",
    "\n",
    "train_full_model = True\n",
    "\n",
    "if (train_full_model):\n",
    "  # Train\n",
    "  full_model, full_train_results = train_model(\n",
    "      full_model,\n",
    "      full_optimizer,\n",
    "      full_method,\n",
    "      train_loader,\n",
    "      DEVICE,\n",
    "      EPOCHS,\n",
    "      save_model=True\n",
    "  )\n",
    "else:\n",
    "  # Load checkpoint\n",
    "  cpt_string = full_method + \"_checkpoint.pth\"\n",
    "  checkpoint = torch.load(cpt_string)\n",
    "  full_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  full_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  results = {}\n",
    "  results['train_time_sec'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "full_eval_results = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    full_method\n",
    ")\n",
    "\n",
    "full_results = {**full_train_results, **full_eval_results}\n",
    "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)"
   ],
   "metadata": {
    "id": "nOh-mU-ycSjn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# MULTI-RUN LoRA / QLoRA / AdaLoRA / DoRA BENCHMARK\n",
    "# ============================================================\n",
    "import torch\n",
    "\n",
    "methods_to_run = [\"LoRA\", \"LoRA+\", \"AdaLoRA\", \"DoRA\"]\n",
    "# results_table = []\n",
    "\n",
    "for lora_method in methods_to_run:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"\ud83d\ude80 Starting Fine-Tuning with {lora_method}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    train_lora_model = True\n",
    "\n",
    "    # ---- Prepare model and optimizer ----\n",
    "    try:\n",
    "        lora_model, lora_optimizer = prepare_model(lora_method)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to prepare {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if train_lora_model:\n",
    "        # ---- Train ----\n",
    "        try:\n",
    "            lora_model, lora_train_results = train_model(\n",
    "                lora_model,\n",
    "                lora_optimizer,\n",
    "                lora_method,\n",
    "                train_loader,\n",
    "                DEVICE,\n",
    "                EPOCHS,\n",
    "                save_model=True\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"\u26a0\ufe0f Skipping {lora_method} (Out of memory)\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Training failed for {lora_method}: {e}\")\n",
    "            continue\n",
    "\n",
    "    else:\n",
    "        # ---- Load checkpoint ----\n",
    "        cpt_string = f\"{lora_method}_checkpoint.pth\"\n",
    "        checkpoint = torch.load(cpt_string)\n",
    "        lora_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        lora_optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        loss = checkpoint[\"loss\"]\n",
    "        lora_train_results = {\"train_time_sec\": 0, \"final_loss\": loss}\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    try:\n",
    "        lora_eval_results = evaluate_model(\n",
    "            lora_model,\n",
    "            val_loader,\n",
    "            f1_metric,\n",
    "            THRESHOLD,\n",
    "            DEVICE,\n",
    "            lora_method\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Evaluation failed for {lora_method}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ---- Merge and store results ----\n",
    "    lora_results = {**lora_train_results, **lora_eval_results}\n",
    "    results_table.append({\"Method\": lora_method, **lora_results})\n",
    "\n",
    "    # ---- Cleanup GPU memory ----\n",
    "    del lora_model\n",
    "    del lora_optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83c\udfc1 ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to DataFrame for nice display (optional)\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results_table)\n",
    "display(results_df)\n"
   ],
   "metadata": {
    "id": "4EfsUIe4rwN9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ebcb8bc-dc23-4434-a5d7-ea0e0b59c61e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "\ud83d\ude80 Starting Fine-Tuning with LoRA\n",
      "======================================================================\n",
      "\n",
      "\ud83e\udde9 Using LoRA\n",
      "trainable params: 907,036 || all params: 125,574,200 || trainable%: 0.7223\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LoRA Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:54<00:00,  4.57it/s, loss=0.113]\n",
      "LoRA Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:53<00:00,  4.58it/s, loss=0.116]\n",
      "LoRA Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:52<00:00,  4.58it/s, loss=0.132]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "| LoRA Evaluation Results |\n",
      "--------------------------------------------------\n",
      "F1 Macro                      : 0.4030\n",
      "F1 Micro                      : 0.5664\n",
      "F1 Weighted                   : 0.5229\n",
      "Precision Macro               : 0.6706\n",
      "Recall Macro                  : 0.3417\n",
      "Hamming Loss                  : 0.0308\n",
      "Exact Match Accuracy          : 0.4425\n",
      "Roc Auc Macro                 : 0.9304\n",
      "Pr Auc Macro                  : 0.5002\n",
      "--------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "\ud83d\ude80 Starting Fine-Tuning with LoRA+\n",
      "======================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2699\ufe0f Using LoRA+\n",
      "trainable params: 907,036 || all params: 125,574,200 || trainable%: 0.7223\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LoRA+ Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:52<00:00,  4.58it/s, loss=0.131]\n",
      "LoRA+ Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:52<00:00,  4.58it/s, loss=0.0979]\n",
      "LoRA+ Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [09:51<00:00,  4.59it/s, loss=0.097]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "| LoRA+ Evaluation Results |\n",
      "--------------------------------------------------\n",
      "F1 Macro                      : 0.4094\n",
      "F1 Micro                      : 0.5626\n",
      "F1 Weighted                   : 0.5233\n",
      "Precision Macro               : 0.6580\n",
      "Recall Macro                  : 0.3457\n",
      "Hamming Loss                  : 0.0304\n",
      "Exact Match Accuracy          : 0.4313\n",
      "Roc Auc Macro                 : 0.9307\n",
      "Pr Auc Macro                  : 0.5033\n",
      "--------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "\ud83d\ude80 Starting Fine-Tuning with AdaLoRA\n",
      "======================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 Using AdaLoRA\n",
      "trainable params: 1,939,804 || all params: 126,607,040 || trainable%: 1.5321\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "AdaLoRA Epoch 1:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 2186/2714 [11:34<02:46,  3.17it/s, loss=0.116]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\\nCOMPARISON OF RESULTS\")\n",
    "print(\"#\" * 60)\n",
    "\n",
    "# Print table header\n",
    "print(f\"| {'Method':<20} | {'Trainable Params (M)':<20} | {'Train Time (s)':<15} | {'Macro F1':<10} |\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Print each row from results_table\n",
    "for result in results_table:\n",
    "    method_name = result[\"Method\"]\n",
    "    trainable_params = result.get(\"trainable_params\", 0.0)\n",
    "    train_time = result.get(\"train_time_sec\", 0.0)\n",
    "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
    "\n",
    "    print(f\"| {method_name:<20} | {trainable_params / 1e6:<20.4f} | {train_time:<15.2f} | {f1_macro:<10.4f} |\")\n",
    "\n",
    "print(\"#\" * 60)\n"
   ],
   "metadata": {
    "id": "5ciWQIO_rzhl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = list(results_print.keys())\n",
    "params = [v[0] for v in results_print.values()]\n",
    "train_times = [v[1] for v in results_print.values()]\n",
    "f1_macros = [v[2] for v in results_print.values()]\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "colors = plt.cm.tab10.colors  # Distinct colors for each method\n",
    "\n",
    "# --- Plot 1: Training Time vs F1 ---\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(train_times[i], f1_macros[i], s=120, label=method, color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(train_times[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Training Time (seconds)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Training Time vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# --- Plot 2: Parameters vs F1 ---\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.scatter(params[i], f1_macros[i], s=120, label=method, color=colors[i % len(colors)], edgecolors='black', linewidth=1.2)\n",
    "    plt.text(params[i]*1.01, f1_macros[i], method, fontsize=9, va='center')\n",
    "plt.xlabel('Trainable Parameters (Millions)', fontsize=11)\n",
    "plt.ylabel('Macro F1 Score', fontsize=11)\n",
    "plt.title('Model Size vs Macro F1', fontsize=13, weight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "vjU0-NLCDbna"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}