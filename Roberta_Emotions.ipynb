{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/QLora/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (PEFT is the modern standard for LoRA)\n",
        "!pip install transformers datasets accelerate evaluate\n",
        "!pip install -U peft\n",
        "!pip install -U bitsandbytes\n",
        "!pip show peft transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBk7G5ubZBy1",
        "outputId": "dfa72831-b356-44c9-ba3e-b97efc945ab0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.10.5)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Name: peft\n",
            "Version: 0.17.1\n",
            "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
            "Home-page: https://github.com/huggingface/peft\n",
            "Author: The HuggingFace team\n",
            "Author-email: benjamin@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
            "Required-by: \n",
            "---\n",
            "Name: transformers\n",
            "Version: 4.57.0\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "NUM_LABELS = 28\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 5e-5\n",
        "FF_LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 16\n",
        "# https://arxiv.org/pdf/2412.12148\n",
        "THRESHOLD = 0.5"
      ],
      "metadata": {
        "id": "_jAa-D4aZDe6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "max_length = 128\n",
        "\n",
        "def tokenize(batch):\n",
        "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
        "    encodings['labels'] = batch['labels']\n",
        "    return encodings\n",
        "\n",
        "ds_encoded = ds.map(tokenize, batched=True)\n",
        "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
        "        if item['labels'] is not None:\n",
        "            for l in item['labels']:\n",
        "                if 0 <= l < NUM_LABELS:\n",
        "                    multi_hot[l] = 1.0\n",
        "        labels.append(multi_hot)\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluation Metric\n",
        "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
      ],
      "metadata": {
        "id": "tbB9oI1PZGHc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, BitsAndBytesConfig\n",
        "from peft import LoraConfig, AdaLoraConfig, get_peft_model, TaskType\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def prepare_model(method: str):\n",
        "    \"\"\"\n",
        "    Prepares a RoBERTa model for fine-tuning using different PEFT strategies:\n",
        "    - \"Full\"      : full fine-tuning\n",
        "    - \"LoRA\"      : standard LoRA\n",
        "    - \"LoRA+\"     : LoRA with Rescaled Stable adaptation\n",
        "    - \"AdaLoRA\"   : Adaptive LoRA (dynamic rank allocation)\n",
        "    - \"DoRA\"      : Weight-decomposed LoRA (Meta 2024)\n",
        "    - \"QLoRA\"     : 4-bit quantized LoRA\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Optional quantization ----------\n",
        "    quantization_config = None\n",
        "    if method == \"QLoRA\":\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "    # ---------- Load base RoBERTa ----------\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=NUM_LABELS,\n",
        "        problem_type=\"multi_label_classification\",\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" if method == \"QLoRA\" else None,\n",
        "    )\n",
        "\n",
        "    # ---------- Choose PEFT variant ----------\n",
        "    if method == \"LoRA\":\n",
        "        config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "        print(\"\\n🧩 Using LoRA\")\n",
        "\n",
        "    elif method == \"LoRA+\":\n",
        "        config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            use_rslora=True,  # LoRA+\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "        print(\"\\n⚙️ Using LoRA+\")\n",
        "\n",
        "    elif method == \"AdaLoRA\":\n",
        "        config = AdaLoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            init_r=8,\n",
        "            target_r=4,\n",
        "            tinit=100,\n",
        "            tfinal=500,\n",
        "            deltaT=10,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "        print(\"\\n🚀 Using AdaLoRA\")\n",
        "\n",
        "    elif method == \"DoRA\":\n",
        "        config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            use_dora=True,  # enables DoRA\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "        print(\"\\n🧠 Using DoRA\")\n",
        "\n",
        "    elif method == \"QLoRA\":\n",
        "        config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "        )\n",
        "        model = get_peft_model(model, config)\n",
        "        print(\"\\n💾 Using QLoRA (4-bit quantized + LoRA)\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n🧱 Full Fine-Tuning (no adapters)\")\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
        "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100%)\")\n",
        "        model.to(DEVICE)\n",
        "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
        "        return model, optimizer\n",
        "\n",
        "    # ---------- Shared setup for PEFT variants ----------\n",
        "    model.print_trainable_parameters()\n",
        "    model.to(DEVICE)\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "1jfdNdbbZHvC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, method: str, train_loader, device, epochs, save_model):\n",
        "    results = {}\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
        "        for batch in loop:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    total_train_time = time.time() - start_time\n",
        "    results['train_time_sec'] = total_train_time\n",
        "\n",
        "    cpt_str = method + \"_checkpoint.pth\"\n",
        "\n",
        "    if save_model:\n",
        "      torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "      }, cpt_str)\n",
        "\n",
        "    return model, results"
      ],
      "metadata": {
        "id": "Kv5EoKDDcwcN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            pred_probs = torch.sigmoid(logits)\n",
        "            pred_labels = (pred_probs > threshold).long()\n",
        "\n",
        "            all_preds.append(pred_labels.cpu().numpy())\n",
        "            all_targets.append(labels.long().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(all_preds, axis=0)\n",
        "    targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    f1_results = {\"f1\": f1_score(targets, preds, average=\"macro\")}\n",
        "    exact_match = np.mean([np.all(p == t) for p, t in zip(preds, targets)])\n",
        "\n",
        "    results = {\n",
        "        'f1_macro': f1_results['f1'],\n",
        "        'exact_match_accuracy': exact_match\n",
        "    }\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"| {method} Evaluation Results |\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Validation Macro F1 Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Validation Exact Match Accuracy: {results['exact_match_accuracy']:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "YMmQhmOoe9Nh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup(full_model=None, full_optimizer=None):\n",
        "    if full_model is not None:\n",
        "        del full_model\n",
        "    if full_optimizer is not None:\n",
        "        del full_optimizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RSq1mQm1sLod"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_table = []\n",
        "\n",
        "print(\"Before FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nEvaluating model before training...\")\n",
        "pretrain_eval = evaluate_model(\n",
        "    full_model,\n",
        "    val_loader,\n",
        "    f1_metric,\n",
        "    THRESHOLD,\n",
        "    DEVICE,\n",
        "    \"Full Fine-Tuning (Before Training)\"\n",
        ")\n",
        "\n",
        "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
        "results_table.append({\"Method\": \"LoRA\", **before_results})\n",
        "\n",
        "cleanup(full_model, full_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24TkGpNProMd",
        "outputId": "0e78ed51-4b1c-4921-d4f1-c988cec70ee2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before FINE-TUNING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧱 Full Fine-Tuning (no adapters)\n",
            "Total Parameters: 124.67M\n",
            "Trainable Parameters: 124.67M (100%)\n",
            "\n",
            "Evaluating model before training...\n",
            "--------------------------------------------------\n",
            "| Full Fine-Tuning (Before Training) Evaluation Results |\n",
            "--------------------------------------------------\n",
            "Validation Macro F1 Score: 0.0493\n",
            "Validation Exact Match Accuracy: 0.0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_method = \"Full Fine-Tuning\"\n",
        "\n",
        "full_model, full_optimizer = prepare_model(full_method)\n",
        "\n",
        "train_full_model = True\n",
        "\n",
        "if (train_full_model):\n",
        "  # Train\n",
        "  full_model, full_train_results = train_model(\n",
        "      full_model,\n",
        "      full_optimizer,\n",
        "      full_method,\n",
        "      train_loader,\n",
        "      DEVICE,\n",
        "      EPOCHS,\n",
        "      save_model=True\n",
        "  )\n",
        "else:\n",
        "  # Load checkpoint\n",
        "  cpt_string = full_method + \"_checkpoint.pth\"\n",
        "  checkpoint = torch.load(cpt_string)\n",
        "  full_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  full_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "  results = {}\n",
        "  results['train_time_sec'] = 0\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "full_eval_results = evaluate_model(\n",
        "    full_model,\n",
        "    val_loader,\n",
        "    f1_metric,\n",
        "    THRESHOLD,\n",
        "    DEVICE,\n",
        "    full_method\n",
        ")\n",
        "\n",
        "full_results = {**full_train_results, **full_eval_results}\n",
        "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
        "\n",
        "cleanup(full_model, full_optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOh-mU-ycSjn",
        "outputId": "b7a53572-7092-4dc6-ee45-9eaa0b4ee595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Full Fine-Tuning Configuration \n",
            "Total Parameters: 355.39M\n",
            "Trainable Parameters: 355.39M (100.00%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Fine-Tuning Epoch 1: 100%|██████████| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.067]\n",
            "Full Fine-Tuning Epoch 2: 100%|██████████| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.0728]\n",
            "Full Fine-Tuning Epoch 3: 100%|██████████| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.0486]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "| Full Fine-Tuning Evaluation Results |\n",
            "--------------------------------------------------\n",
            "Validation Macro F1 Score: 0.4511\n",
            "Validation Exact Match Accuracy: 0.4751\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MULTI-RUN LoRA / QLoRA / AdaLoRA / DoRA BENCHMARK\n",
        "# ============================================================\n",
        "import torch\n",
        "\n",
        "methods_to_run = [\"LoRA\", \"LoRA+\", \"AdaLoRA\", \"DoRA\"]\n",
        "results_table = []\n",
        "\n",
        "for lora_method in methods_to_run:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🚀 Starting Fine-Tuning with {lora_method}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    train_lora_model = True\n",
        "\n",
        "    # ---- Prepare model and optimizer ----\n",
        "    try:\n",
        "        lora_model, lora_optimizer = prepare_model(lora_method)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to prepare {lora_method}: {e}\")\n",
        "        continue\n",
        "\n",
        "    if train_lora_model:\n",
        "        # ---- Train ----\n",
        "        try:\n",
        "            lora_model, lora_train_results = train_model(\n",
        "                lora_model,\n",
        "                lora_optimizer,\n",
        "                lora_method,\n",
        "                train_loader,\n",
        "                DEVICE,\n",
        "                EPOCHS,\n",
        "                save_model=True\n",
        "            )\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            print(f\"⚠️ Skipping {lora_method} (Out of memory)\")\n",
        "            torch.cuda.empty_cache()\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Training failed for {lora_method}: {e}\")\n",
        "            continue\n",
        "\n",
        "    else:\n",
        "        # ---- Load checkpoint ----\n",
        "        cpt_string = f\"{lora_method}_checkpoint.pth\"\n",
        "        checkpoint = torch.load(cpt_string)\n",
        "        lora_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        lora_optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        epoch = checkpoint[\"epoch\"]\n",
        "        loss = checkpoint[\"loss\"]\n",
        "        lora_train_results = {\"train_time_sec\": 0, \"final_loss\": loss}\n",
        "\n",
        "    # ---- Evaluate ----\n",
        "    try:\n",
        "        lora_eval_results = evaluate_model(\n",
        "            lora_model,\n",
        "            val_loader,\n",
        "            f1_metric,\n",
        "            THRESHOLD,\n",
        "            DEVICE,\n",
        "            lora_method\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Evaluation failed for {lora_method}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # ---- Merge and store results ----\n",
        "    lora_results = {**lora_train_results, **lora_eval_results}\n",
        "    results_table.append({\"Method\": lora_method, **lora_results})\n",
        "\n",
        "    # ---- Cleanup GPU memory ----\n",
        "    del lora_model\n",
        "    del lora_optimizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🏁 ALL EXPERIMENTS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Convert to DataFrame for nice display (optional)\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results_table)\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "4EfsUIe4rwN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebf00f1-fe3e-4938-9c2c-5697183a48a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "🚀 Starting Fine-Tuning with LoRA\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧩 Using LoRA\n",
            "trainable params: 907,036 || all params: 125,574,200 || trainable%: 0.7223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LoRA Epoch 1: 100%|██████████| 2714/2714 [10:03<00:00,  4.49it/s, loss=0.0652]\n",
            "LoRA Epoch 2:  35%|███▍      | 939/2714 [03:30<06:37,  4.46it/s, loss=0.117]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_print = {}\n",
        "\n",
        "for result in results_table:\n",
        "    method_name = result[\"Method\"]\n",
        "    model = prepare_model(method_name)[0]\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    train_time = result.get(\"train_time_sec\", 0.0)\n",
        "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
        "\n",
        "    results_print[method_name] = [trainable_params/1e6, train_time, f1_macro]\n",
        "\n",
        "print(\"\\n\\nCOMPARISON OF RESULTS\")\n",
        "print(\"#\"*60)\n",
        "\n",
        "print(f\"| {'Method':<20} | {'Trainable Params (M)':<20} | {'Train Time (s)':<15} | {'Macro F1':<10} |\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for key, value in results_print.items():\n",
        "    print(f\"|{key:<20}|\", end = \" \")\n",
        "    for x in value:\n",
        "      print(f\"{x:20.4f}\", end = \" \")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# print(f\"| {method_name:<20} | {trainable_params/1e6:<20.2f}M | {train_time:<15.2f} | {f1_macro:<10.4f} |\")\n",
        "\n",
        "print(\"#\" * 60)"
      ],
      "metadata": {
        "id": "5ciWQIO_rzhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9acf489-ebab-4df8-a839-d3f7eee80060"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LoRA Configuration\n",
            "trainable params: 907,036 || all params: 125,574,200 || trainable%: 0.7223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LoRA Configuration\n",
            "trainable params: 907,036 || all params: 125,574,200 || trainable%: 0.7223\n",
            "\n",
            "\n",
            "COMPARISON OF RESULTS\n",
            "############################################################\n",
            "| Method               | Trainable Params (M) | Train Time (s)  | Macro F1   |\n",
            "---------------------------------------------------------------------------\n",
            "|LoRA                |               0.9070            1818.6377               0.3987 \n",
            "\n",
            "############################################################\n"
          ]
        }
      ]
    }
  ]
}