{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNBNLQOovsZOpVe+Xj6nFCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/main/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (PEFT is the modern standard for LoRA)\n",
        "!pip install transformers datasets accelerate evaluate peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBk7G5ubZBy1",
        "outputId": "edf80fbb-37b1-47bb-9348-266b0700c90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"roberta-large\"\n",
        "NUM_LABELS = 28\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 5e-5\n",
        "FF_LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 16\n",
        "THRESHOLD = 0.5"
      ],
      "metadata": {
        "id": "_jAa-D4aZDe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "max_length = 128\n",
        "\n",
        "def tokenize(batch):\n",
        "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
        "    encodings['labels'] = batch['labels']\n",
        "    return encodings\n",
        "\n",
        "ds_encoded = ds.map(tokenize, batched=True)\n",
        "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
        "        if item['labels'] is not None:\n",
        "            for l in item['labels']:\n",
        "                if 0 <= l < NUM_LABELS:\n",
        "                    multi_hot[l] = 1.0\n",
        "        labels.append(multi_hot)\n",
        "\n",
        "    labels = torch.stack(labels)\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluation Metric\n",
        "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
      ],
      "metadata": {
        "id": "tbB9oI1PZGHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model(method: str):\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=NUM_LABELS,\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    )\n",
        "\n",
        "    if method == \"LoRA\":\n",
        "        peft_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"query\", \"value\"]\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "        print(\"\\n LoRA Configuration\")\n",
        "        model.print_trainable_parameters()\n",
        "        model.to(DEVICE)\n",
        "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Add loRa varient here, i like QRLora maybe\n",
        "\n",
        "    else:\n",
        "        print(\"\\n Full Fine-Tuning Configuration \")\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
        "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100.00%)\")\n",
        "\n",
        "        model.to(DEVICE)\n",
        "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
        "\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "1jfdNdbbZHvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, method: str, train_loader, device, epochs):\n",
        "    results = {}\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
        "        for batch in loop:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    total_train_time = time.time() - start_time\n",
        "    results['train_time_sec'] = total_train_time\n",
        "\n",
        "    return model, results"
      ],
      "metadata": {
        "id": "Kv5EoKDDcwcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            pred_probs = torch.sigmoid(logits)\n",
        "            pred_labels = (pred_probs > threshold).long()\n",
        "\n",
        "            all_preds.append(pred_labels.cpu().numpy())\n",
        "            all_targets.append(labels.long().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(all_preds, axis=0)\n",
        "    targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    f1_results = {\"f1\": f1_score(targets, preds, average=\"macro\")}\n",
        "    exact_match = np.mean([np.all(p == t) for p, t in zip(preds, targets)])\n",
        "\n",
        "    results = {\n",
        "        'f1_macro': f1_results['f1'],\n",
        "        'exact_match_accuracy': exact_match\n",
        "    }\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"| {method} Evaluation Results |\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Validation Macro F1 Score: {results['f1_macro']:.4f}\")\n",
        "    print(f\"Validation Exact Match Accuracy: {results['exact_match_accuracy']:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "YMmQhmOoe9Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup(full_model=None, full_optimizer=None):\n",
        "    if full_model is not None:\n",
        "        del full_model\n",
        "    if full_optimizer is not None:\n",
        "        del full_optimizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RSq1mQm1sLod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_table = []\n",
        "\n",
        "print(\"Before FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nEvaluating model before training...\")\n",
        "pretrain_eval = evaluate_model(\n",
        "    full_model,\n",
        "    val_loader,\n",
        "    f1_metric,\n",
        "    THRESHOLD,\n",
        "    DEVICE,\n",
        "    \"Full Fine-Tuning (Before Training)\"\n",
        ")\n",
        "\n",
        "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
        "results_table.append({\"Method\": \"LoRA\", **before_results})\n",
        "\n",
        "cleanup(full_model, full_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24TkGpNProMd",
        "outputId": "f3382ed9-5617-432d-fbdb-f1a9093eab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before FINE-TUNING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Full Fine-Tuning Configuration ---\n",
            "Total Parameters: 355.39M\n",
            "Trainable Parameters: 355.39M (100.00%)\n",
            "\n",
            "Evaluating model before training...\n",
            "--------------------------------------------------\n",
            "| Full Fine-Tuning (Before Training) Evaluation Results |\n",
            "--------------------------------------------------\n",
            "Validation Macro F1 Score: 0.0514\n",
            "Validation Exact Match Accuracy: 0.0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
        "\n",
        "# Train\n",
        "full_model, full_train_results = train_model(\n",
        "    full_model,\n",
        "    full_optimizer,\n",
        "    \"Full Fine-Tuning\",\n",
        "    train_loader,\n",
        "    DEVICE,\n",
        "    EPOCHS\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "full_eval_results = evaluate_model(\n",
        "    full_model,\n",
        "    val_loader,\n",
        "    f1_metric,\n",
        "    THRESHOLD,\n",
        "    DEVICE,\n",
        "    \"Full Fine-Tuning\"\n",
        ")\n",
        "\n",
        "full_results = {**full_train_results, **full_eval_results}\n",
        "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
        "\n",
        "cleanup(full_model, full_optimizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOh-mU-ycSjn",
        "outputId": "965120bf-9dac-41f7-c5ec-62eb9a2474fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Full Fine-Tuning Configuration ---\n",
            "Total Parameters: 355.39M\n",
            "Trainable Parameters: 355.39M (100.00%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Full Fine-Tuning Epoch 1: 100%|██████████| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.135]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "| Full Fine-Tuning Evaluation Results |\n",
            "--------------------------------------------------\n",
            "Validation Macro F1 Score: 0.3387\n",
            "Validation Exact Match Accuracy: 0.4206\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LoRA FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "lora_model, lora_optimizer = prepare_model(\"LoRA\")\n",
        "\n",
        "# Train\n",
        "lora_model, lora_train_results = train_model(\n",
        "    lora_model,\n",
        "    lora_optimizer,\n",
        "    \"LoRA\",\n",
        "    train_loader,\n",
        "    DEVICE,\n",
        "    EPOCHS\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "lora_eval_results = evaluate_model(\n",
        "    lora_model,\n",
        "    val_loader,\n",
        "    f1_metric,\n",
        "    THRESHOLD,\n",
        "    DEVICE,\n",
        "    \"LoRA\"\n",
        ")\n",
        "\n",
        "lora_results = {**lora_train_results, **lora_eval_results}\n",
        "results_table.append({\"Method\": \"LoRA\", **lora_results})\n",
        "\n",
        "cleanup(full_model, full_optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EfsUIe4rwN9",
        "outputId": "39dea8da-0605-4225-b560-11cb7abcfc0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA FINE-TUNING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA Configuration ---\n",
            "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LoRA Epoch 1: 100%|██████████| 2714/2714 [07:55<00:00,  5.71it/s, loss=0.101]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "| LoRA Evaluation Results |\n",
            "--------------------------------------------------\n",
            "Validation Macro F1 Score: 0.3501\n",
            "Validation Exact Match Accuracy: 0.3959\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"COMPARISON OF RESULTS\")\n",
        "print(\"#\"*60)\n",
        "\n",
        "print(f\"| {'Method':<20} | {'Trainable Params':<20} | {'Train Time (s)':<15} | {'Macro F1':<10} |\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for result in results_table:\n",
        "    method_name = result[\"Method\"]\n",
        "    model = prepare_model(method_name)[0]\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    train_time = result.get(\"train_time_sec\", 0.0)\n",
        "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
        "\n",
        "    print(f\"| {method_name:<20} | {trainable_params/1e6:<20.2f}M | {train_time:<15.2f} | {f1_macro:<10.4f} |\")\n",
        "\n",
        "print(\"#\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ciWQIO_rzhl",
        "outputId": "3b4ebca0-1b46-4e21-f191-76218b540a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "COMPARISON OF RESULTS\n",
            "############################################################\n",
            "| Method               | Trainable Params     | Train Time (s)  | Macro F1   |\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA Configuration ---\n",
            "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n",
            "| LoRA                 | 1.86                M | 0.00            | 0.0514     |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Full Fine-Tuning Configuration ---\n",
            "Total Parameters: 355.39M\n",
            "Trainable Parameters: 355.39M (100.00%)\n",
            "| Full Fine-Tuning     | 355.39              M | 716.02          | 0.3387     |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LoRA Configuration ---\n",
            "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n",
            "| LoRA                 | 1.86                M | 475.21          | 0.3501     |\n",
            "############################################################\n"
          ]
        }
      ]
    }
  ]
}