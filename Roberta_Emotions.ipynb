{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "authorship_tag": "ABX9TyNZ6rRxewea8BCFDGpxijov",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dominickstephens/aLoRa/blob/main/Roberta_Emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install necessary libraries (PEFT is the modern standard for LoRA)\n",
    "!pip install transformers datasets accelerate evaluate peft"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBk7G5ubZBy1",
    "outputId": "8d55e813-da46-4aa7-e339-65b7dae782b1"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"roberta-large\"\n",
    "NUM_LABELS = 28\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "FF_LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 16\n",
    "# https://arxiv.org/pdf/2412.12148\n",
    "THRESHOLD = 0.5"
   ],
   "metadata": {
    "id": "_jAa-D4aZDe6"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "max_length = 128\n",
    "\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    encodings['labels'] = batch['labels']\n",
    "    return encodings\n",
    "\n",
    "ds_encoded = ds.map(tokenize, batched=True)\n",
    "ds_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        multi_hot = torch.zeros(NUM_LABELS, dtype=torch.float)\n",
    "        if item['labels'] is not None:\n",
    "            for l in item['labels']:\n",
    "                if 0 <= l < NUM_LABELS:\n",
    "                    multi_hot[l] = 1.0\n",
    "        labels.append(multi_hot)\n",
    "\n",
    "    labels = torch.stack(labels)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(ds_encoded['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds_encoded['validation'], batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluation Metric\n",
    "f1_metric = evaluate.load(\"f1\", config=\"multilabel\")"
   ],
   "metadata": {
    "id": "tbB9oI1PZGHc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "cfa9baffd28d4eaa94d9f9461228566d",
      "d565bf58b4c540a4843bd818e42f4eb7",
      "715bd17016894deba719294e658d0796",
      "711e1e9696434dcf9d60a6895fcf581a",
      "633529b8d3f54154924ea77594bf936e",
      "4e62fb1bea47413482443d480f4d57ae",
      "42a3c8bcf6184bb89dccc30bc3078ac5",
      "c65f0afa0a644ef7a6db283bf8bd0989",
      "6f1fbb3800ca475b96e162af2dc0fa4f",
      "779ff91476634e8b9b3c95c1cf0134cd",
      "25d9e4d439fc436f85ab0e8b37961d0e",
      "af3eb62d98d748cbaedf9fc51e604e9e",
      "09b7caf7204649699156bf09c5a385f1",
      "2f46cffa76e44979a1f83768a881cf93",
      "469b8469a9164548b64dedfa95946d59",
      "360614ec82fa410486223e32ad20460a",
      "c6338bc1819b4348a5cb8eac96d2e49b",
      "d372078f748b43878bc6f38eaf0f71fa",
      "3ff2120b7ceb43a8b21f8a0dd2c43256",
      "3d8911c246314d2c88f5a522d77ee708",
      "2c031e6635004205bfd48189ac067f1c",
      "477265c0b6be4ae7adc149642ca2d196",
      "a6ae7e20ce5944e48fd04e5b1f51710c",
      "98f4a0fe9a164d68a99c239fd07b6491",
      "9f50a070c6cc40b09b7207b463efaf41",
      "55916cd61ce14118855bd2a704785654",
      "51672f3f803e495286a9e1d3fa778017",
      "bdf43a90c146414b8d13eb29b92c12ec",
      "e1dc4faab2474e76ab108855411192c1",
      "bc6fc6cdd0b840c0aa8202cef6401f44",
      "e1ca240ef7014a65954caaae23cbcc1f",
      "18fbdec792c34a8aaafa191f1564614e",
      "5eaf030c2a1f406294579dd2f9c6c8f5",
      "57e87a19371d4a94a5bf17ab1fb08f1a",
      "c81bcbb36b4a4038ba34bf0b9dbe39a1",
      "a9b1b6237b3b44b3bc57cfde329f1f15",
      "387f8855cdbb40baa390460f74545785",
      "120180fcbf034608a2fdab88516eaa36",
      "325eaf2ae3584e3ea9a8e9162c965dcd",
      "a83bb8dea2a14dd69ef29c3030b7cb91",
      "62cbf65bbb9e4eb8b282a048a7545ed4",
      "0b810c72b8ff4df68ea5f4dc37cf1f3b",
      "2031d2e25b4a46b5800435641ab63ffe",
      "55ea76d2d57644698bb437cfb205c295",
      "fdbf394d0cc044a089f0814e8e82d614",
      "7ac071411bc14f89bf33270e7014ede7",
      "efcc4384221741db96a0de1f81152e39",
      "00c5e31a31dd4e348fe7d5024fff221a",
      "d7c9b74371fe43c196ac11643ac51d5c",
      "bbf43085c5d0454eb9d57aabbb553abd",
      "2ec4649164914469b27c3ade6d7b894d",
      "77bc8145e94d4923abb6a53ea971a019",
      "ff994b050dec455a95965a1fcabe1244",
      "6c7ad936543b41bb8e560e0f565cbad8",
      "fa6af67c79fb4a9bb9efcf1c449c15d2",
      "46ccff960c5e44a991b27e9bd171ee4e",
      "966041a8e4a043d48d85ec459c08212a",
      "09ab47b5fbfd4b9f8ad1aef426be74cf",
      "99c2fc42ed104fb5a3b016837aa22ba3",
      "159da895f5774efd8bebd694e10e85c5",
      "731ea84e67994f16a9cf803fa0374ae8",
      "6e59d24a34634608a75b5519aec60bbc",
      "26e99f2c2b3143438d31086abbfb553e",
      "b71aae211bdc42bfa0c54bf782d33a9e",
      "d3553456709a4bbcabc06ce81d43cbf6",
      "d575b02d201045f182baa06de17301e8",
      "16de8e67f693489c815936e2a0fbff8f",
      "f49404b7bd9e46e8b690ce6856a7df5a",
      "819898590e5b47729b2a12b0d378b5ff",
      "a88db415ec1f4725a0eb9d9b09da8d0b",
      "6898a07901ec42a6b98f69e1b8e098dc",
      "aed3c403741d4781ab4511a0a59d9b70",
      "c17351a22b404396ab0c453e75d3b1ae",
      "3236aaaaa2e54565b410bcbe0497c1fe",
      "27a414db8b384fc1a64e424fc454d3f3",
      "51ca68af95c44840a6cef025603e2565",
      "d588d7258ff148fba7f8cd65407af1ea",
      "96eef52c1bef4477996587acb9c92c00",
      "0d26129bee6c498cbdac12c75841af15",
      "7d6f4ce5787e4e74b260a0677376e575",
      "806267979e374efdb9b740a1189b8448",
      "7d5a17c6541d4949985e928d60dc1cdf",
      "dbd8487dbb6f473e915f7a71d153f43b",
      "7c12d8d4448a4cf892b5ce6abbefcd4d",
      "d7a05a1ee257462e94ff40b9b8bff9bb",
      "fca019dd26be44e5aaaae04e1f83b07f",
      "611a05cff9bf4ac597b33294fb8f9a1e",
      "d65a27a92a8f45e3b1d4d02b2b040fbf",
      "d0e5beeb6b7b419eb6bbb57b28017985",
      "1be5a0f0268c4439ad4e97ef2ef206b3",
      "a4436a0a8ad74dc781277034d4a560f4",
      "6cd6459dc727406084c795fbf0ad48a9",
      "0fbe2cf0ffd74edfb83e70301e9d4766",
      "6f8be792f6184086a5a7ac9aec54a9bd",
      "e2bc7de258e943788ce4fca264bf3cb4",
      "c2d7c3243a6c4a6dbd68b2e2a5a42727",
      "05c91c415b84405cb408bcd4e6614232",
      "a14a046ce1784ba88c06fa6cda0db024",
      "3f29789352a249ba82bd44e721055541",
      "8d98f0df7e7f4fd5b7117427c342ed7e",
      "08787c3f28a04e238ecb930b2baa4ccd",
      "a8be14c162ae4f53b34e802234df263d",
      "d1401dcf301748408bc136fcb5068e2e",
      "18bc83d6369946008e0f6dec41e5d009",
      "fb4fe20b267d4467928a740d5bc5ca31",
      "0e5d536cb3504448b67289f23650e76c",
      "e37489e13c1948d2bad809927613b113",
      "74fee1116c3c42de80d160ce55c54b82",
      "043d3dde50f540409f3de8ffab09809f",
      "b6020ebd31394d43835ace86b6df7884",
      "d0c44f9b25ea43f3a411286681547cb2",
      "0889d05d59b24dcb8a65af48cfa90bcb",
      "9ead1eeb1def40139c1a1f360c634061",
      "a45cd640437c4bce84dccc0d0c98973f",
      "95fb4f8dd23a405988d0e2f945cb2021",
      "b1536c5d0a714d35b41ad9e533b44b76",
      "f3d4a3cee0994fe8b39abc95ad3d51ad",
      "bed892fdaf58468aad5a75419658921e",
      "7fbb734e25904c67a1e8d635cb89c9ba",
      "29503115b44941aa91ab39c8a7ecc6bc",
      "2fde1b7f9aab41669035f020908a1a5a",
      "928f45209375496ca5db7d69c1d76723",
      "6fae79a2ff2f45d495361ea0c939cf98",
      "169ca54e17b94a9eb997dd0fac196bbc",
      "a6eb537f326c48d5bee24d5abb2e7fca",
      "cb901b3b574944099f1d76d67df342b8",
      "acc6a8e0a6c349fc9b9bdfaf218426f5",
      "67e8c31cbd9b4b47a586508c80ed2476",
      "9a5fc28dd8044371a1ec86a16a62dba2",
      "a840d7bab831475daee4ebf6496b9a1c",
      "ef66cf9f9ed84caf8e1ffc0fc089d97a",
      "dda706881f5d4c88abbef33aef8512ee",
      "6900a326a61b42c9b438600b86ba278a",
      "86f47a3ad4f248fc877b2ea5339cb983",
      "1969a99da51c4a44ac9cc6bd95299c80",
      "d1fc7b56cb1748af97cb499c7cb2326a",
      "00771ea4a2f7411aac31470631427743",
      "dc778657aeb34956b7441a038b2db9ae",
      "3fb3f5d4957a42f98aec2cdcb5648da0",
      "3ff0f052dbe94d7fbff90cc85625297b",
      "05258e0a2ba248bcadbc2486636348db",
      "2f75d93fd5714bee9f2d5261fd9b0225",
      "651ef89a3a7242c1885e2f5f0d8f2795",
      "b0e88f3a07a94626893dcfae744d9cbd",
      "f62e6b0fe2e54324b10cee22b8d1088c",
      "91f85ec35eb04c6383f88d1505cf50aa",
      "f7b5d9dc964240c6a29acdd54ba43378",
      "dcd2b1ece48c445d85349983887d74fd",
      "1473b2e342b14a37ac012f21abd168c6",
      "694458abd8d64362ac5248d1eca95b4b",
      "3389a383f7154500991131dbe6b3f248",
      "6e60ee4d2be8418590407d99a7051815",
      "c86f8de9937d46c1a0b2b8bcb483cd10",
      "6ffe7a9620954dabbcd7621580d8a810",
      "8fef705638104ba5ad90fb66c743f720",
      "e8312c71bb1f4b2594dbb1d769b588c8",
      "a50e9a2f4eaa4522aefe5ab5845ac574",
      "5a64888659554fcebe9695b0270c41b0",
      "f9f5e199a99642dead1d7ff1c925c9b7",
      "a8469d99c29c46388aeb5d11d33d6590",
      "7731ea0d15a34eca9daa3d8825816406",
      "2375ee8f467f40f79cbe7b062ecfceaa",
      "94290a08be99475e9a6ee1240f44a6de",
      "72b83690919f49558d76302e725339e3",
      "a6d0af719ec34ea2a26fb772e4916a38",
      "4a2e212368604b84b38f35cb4365bbf7",
      "a6399931176f4c5cadb12267173cb2db",
      "4cefa046db8a4e78950e372c6355de96",
      "d25f5a002e41426ba81d19c01569962a",
      "6a4fa83b6a5f4305b14f26cbd8b061d0",
      "7f0b39bd6043470aaa88838fed8bc549",
      "a65f0470793c42fba091b4e36930f7a2",
      "49641054b5d24388a8c55ad396dbed52",
      "bdc72b8b2feb4f669732292580549d74",
      "efe053cfa9ef47b8a51569023771ab5f",
      "9e00558aa0a44f45a64201bcdb4c0aa8"
     ]
    },
    "outputId": "a8b5a847-8231-4be3-f5c3-268a65afe273"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfa9baffd28d4eaa94d9f9461228566d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af3eb62d98d748cbaedf9fc51e604e9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/validation-00000-of-00001.par(\u2026):   0%|          | 0.00/350k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6ae7e20ce5944e48fd04e5b1f51710c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57e87a19371d4a94a5bf17ab1fb08f1a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdbf394d0cc044a089f0814e8e82d614"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46ccff960c5e44a991b27e9bd171ee4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16de8e67f693489c815936e2a0fbff8f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96eef52c1bef4477996587acb9c92c00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0e5beeb6b7b419eb6bbb57b28017985"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d98f0df7e7f4fd5b7117427c342ed7e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0c44f9b25ea43f3a411286681547cb2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "928f45209375496ca5db7d69c1d76723"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/43410 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6900a326a61b42c9b438600b86ba278a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0e88f3a07a94626893dcfae744d9cbd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fef705638104ba5ad90fb66c743f720"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a2e212368604b84b38f35cb4365bbf7"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_model(method: str):\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "    if method == \"LoRA\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"]\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        print(\"\\n LoRA Configuration\")\n",
    "        model.print_trainable_parameters()\n",
    "        model.to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Add loRa varient here, i like QRLora maybe\n",
    "\n",
    "    else:\n",
    "        print(\"\\n Full Fine-Tuning Configuration \")\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total Parameters: {total_params / 1e6:.2f}M\")\n",
    "        print(f\"Trainable Parameters: {trainable_params / 1e6:.2f}M (100.00%)\")\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=FF_LEARNING_RATE)\n",
    "\n",
    "    return model, optimizer"
   ],
   "metadata": {
    "id": "1jfdNdbbZHvC"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, optimizer, method: str, train_loader, device, epochs, save_model):\n",
    "    results = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, leave=True, desc=f\"{method} Epoch {epoch+1}\")\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    total_train_time = time.time() - start_time\n",
    "    results['train_time_sec'] = total_train_time\n",
    "\n",
    "    cpt_str = method + \"_checkpoint.pth\"\n",
    "\n",
    "    if save_model:\n",
    "      torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'loss': loss,\n",
    "      }, cpt_str)\n",
    "\n",
    "    return model, results"
   ],
   "metadata": {
    "id": "Kv5EoKDDcwcN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, val_loader, f1_metric, threshold, device, method: str):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            pred_probs = torch.sigmoid(logits)\n",
    "            pred_labels = (pred_probs > threshold).long()\n",
    "\n",
    "            all_preds.append(pred_labels.cpu().numpy())\n",
    "            all_targets.append(labels.long().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    f1_results = {\"f1\": f1_score(targets, preds, average=\"macro\")}\n",
    "    exact_match = np.mean([np.all(p == t) for p, t in zip(preds, targets)])\n",
    "\n",
    "    results = {\n",
    "        'f1_macro': f1_results['f1'],\n",
    "        'exact_match_accuracy': exact_match\n",
    "    }\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"| {method} Evaluation Results |\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Validation Macro F1 Score: {results['f1_macro']:.4f}\")\n",
    "    print(f\"Validation Exact Match Accuracy: {results['exact_match_accuracy']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "YMmQhmOoe9Nh"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cleanup(full_model=None, full_optimizer=None):\n",
    "    if full_model is not None:\n",
    "        del full_model\n",
    "    if full_optimizer is not None:\n",
    "        del full_optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "RSq1mQm1sLod"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results_table = []\n",
    "\n",
    "print(\"Before FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "full_model, full_optimizer = prepare_model(\"Full Fine-Tuning\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating model before training...\")\n",
    "pretrain_eval = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    \"Full Fine-Tuning (Before Training)\"\n",
    ")\n",
    "\n",
    "before_results = {\"train_time_sec\": 0.0, **pretrain_eval}\n",
    "results_table.append({\"Method\": \"LoRA\", **before_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347,
     "referenced_widgets": [
      "6024f05769a949ac98e612288a5bd38a",
      "61525e7d43d04df7948df214b916799c",
      "0380e998ee0a422c9292d81796089f25",
      "2fb6e60fa2364e4a8a8d1814c9f4797d",
      "0e9a2b7fdd7d4700a68b438511fba1a3",
      "d6e3d148385b407bb6bfdf7a72228b46",
      "c40baec447e94b079a75fdccb1516890",
      "21e9631778cc4fec94eeb69cd9654480",
      "c5c335837a634635a636f89066b8c4b4",
      "81024ca5c99048e597e761f2569ba9d7",
      "9b6b740c4ec54d4983f842b89c3577a3"
     ]
    },
    "id": "24TkGpNProMd",
    "outputId": "5b74901d-f55e-42ba-9e01-cc75349aa527"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before FINE-TUNING\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6024f05769a949ac98e612288a5bd38a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Full Fine-Tuning Configuration \n",
      "Total Parameters: 355.39M\n",
      "Trainable Parameters: 355.39M (100.00%)\n",
      "\n",
      "Evaluating model before training...\n",
      "--------------------------------------------------\n",
      "| Full Fine-Tuning (Before Training) Evaluation Results |\n",
      "--------------------------------------------------\n",
      "Validation Macro F1 Score: 0.0165\n",
      "Validation Exact Match Accuracy: 0.0000\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "full_method = \"Full Fine-Tuning\"\n",
    "\n",
    "full_model, full_optimizer = prepare_model(full_method)\n",
    "\n",
    "train_full_model = True\n",
    "\n",
    "if (train_full_model):\n",
    "  # Train\n",
    "  full_model, full_train_results = train_model(\n",
    "      full_model,\n",
    "      full_optimizer,\n",
    "      full_method,\n",
    "      train_loader,\n",
    "      DEVICE,\n",
    "      EPOCHS,\n",
    "      save_model=True\n",
    "  )\n",
    "else:\n",
    "  # Load checkpoint\n",
    "  cpt_string = full_method + \"_checkpoint.pth\"\n",
    "  checkpoint = torch.load(cpt_string)\n",
    "  full_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  full_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  results = {}\n",
    "  results['train_time_sec'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "full_eval_results = evaluate_model(\n",
    "    full_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    full_method\n",
    ")\n",
    "\n",
    "full_results = {**full_train_results, **full_eval_results}\n",
    "results_table.append({\"Method\": \"Full Fine-Tuning\", **full_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOh-mU-ycSjn",
    "outputId": "b7a53572-7092-4dc6-ee45-9eaa0b4ee595"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Full Fine-Tuning Configuration \n",
      "Total Parameters: 355.39M\n",
      "Trainable Parameters: 355.39M (100.00%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Full Fine-Tuning Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.067]\n",
      "Full Fine-Tuning Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.0728]\n",
      "Full Fine-Tuning Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [11:56<00:00,  3.79it/s, loss=0.0486]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "| Full Fine-Tuning Evaluation Results |\n",
      "--------------------------------------------------\n",
      "Validation Macro F1 Score: 0.4511\n",
      "Validation Exact Match Accuracy: 0.4751\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lora_method = \"LoRA\"\n",
    "train_lora_model = True\n",
    "\n",
    "print(\"LoRA FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "lora_model, lora_optimizer = prepare_model(lora_method)\n",
    "\n",
    "if (train_lora_model):\n",
    "\n",
    "  # Train\n",
    "  lora_model, lora_train_results = train_model(\n",
    "      lora_model,\n",
    "      lora_optimizer,\n",
    "      lora_method,\n",
    "      train_loader,\n",
    "      DEVICE,\n",
    "      EPOCHS,\n",
    "      save_model=True\n",
    "  )\n",
    "else:\n",
    "  # Load checkpoint\n",
    "  cpt_string = lora_method + \"_checkpoint.pth\"\n",
    "  checkpoint = torch.load(cpt_string)\n",
    "  lora_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  lora_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "  results = {}\n",
    "  results['train_time_sec'] = 0\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "lora_eval_results = evaluate_model(\n",
    "    lora_model,\n",
    "    val_loader,\n",
    "    f1_metric,\n",
    "    THRESHOLD,\n",
    "    DEVICE,\n",
    "    lora_method\n",
    ")\n",
    "\n",
    "lora_results = {**lora_train_results, **lora_eval_results}\n",
    "results_table.append({\"Method\": \"LoRA\", **lora_results})\n",
    "\n",
    "cleanup(full_model, full_optimizer)"
   ],
   "metadata": {
    "id": "4EfsUIe4rwN9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ad2b365-38f1-4c85-e23c-848605a5616f"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LoRA FINE-TUNING\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " LoRA Configuration\n",
      "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LoRA Epoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [07:55<00:00,  5.71it/s, loss=0.0898]\n",
      "LoRA Epoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [07:55<00:00,  5.71it/s, loss=0.0831]\n",
      "LoRA Epoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2714/2714 [07:54<00:00,  5.72it/s, loss=0.0371]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------------------------------------\n",
      "| LoRA Evaluation Results |\n",
      "--------------------------------------------------\n",
      "Validation Macro F1 Score: 0.4452\n",
      "Validation Exact Match Accuracy: 0.4394\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "results_print = {}\n",
    "\n",
    "for result in results_table:\n",
    "    method_name = result[\"Method\"]\n",
    "    model = prepare_model(method_name)[0]\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    train_time = result.get(\"train_time_sec\", 0.0)\n",
    "    f1_macro = result.get(\"f1_macro\", 0.0)\n",
    "\n",
    "    results_print[method_name] = [trainable_params/1e6, train_time, f1_macro]\n",
    "\n",
    "print(\"\\n\\nCOMPARISON OF RESULTS\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "print(f\"| {'Method':<20} | {'Trainable Params (M)':<20} | {'Train Time (s)':<15} | {'Macro F1':<10} |\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for key, value in results_print.items():\n",
    "    print(f\"|{key:<20}|\", end = \" \")\n",
    "    for x in value:\n",
    "      print(f\"{x:20.4f}\", end = \" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# print(f\"| {method_name:<20} | {trainable_params/1e6:<20.2f}M | {train_time:<15.2f} | {f1_macro:<10.4f} |\")\n",
    "\n",
    "print(\"#\" * 60)"
   ],
   "metadata": {
    "id": "5ciWQIO_rzhl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "322bb9c4-c4e4-4992-c966-6aac61924878"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " LoRA Configuration\n",
      "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Full Fine-Tuning Configuration \n",
      "Total Parameters: 355.39M\n",
      "Trainable Parameters: 355.39M (100.00%)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " LoRA Configuration\n",
      "trainable params: 1,864,732 || all params: 357,253,176 || trainable%: 0.5220\n",
      "\n",
      "\n",
      "COMPARISON OF RESULTS\n",
      "############################################################\n",
      "| Method               | Trainable Params (M) | Train Time (s)  | Macro F1   |\n",
      "---------------------------------------------------------------------------\n",
      "|LoRA                |               1.8647            1424.7488               0.4452 \n",
      "\n",
      "|Full Fine-Tuning    |             355.3884            2149.0254               0.4511 \n",
      "\n",
      "############################################################\n"
     ]
    }
   ]
  }
 ]
}